% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother


% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}



\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Predicting Wine Quality using Random Forest Classifier},
  pdfauthor={Junli Liu, Purity Jangaya, Luis Alonso Alvarez \& Jimmy Wang},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Predicting Wine Quality using Random Forest Classifier}
\author{Junli Liu, Purity Jangaya, Luis Alonso Alvarez \& Jimmy Wang}
\date{2025-11-20}
\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{4}
\tableofcontents
}

\subsection{Summary}\label{summary}

This project implements a Random Forest classifier to predict wine
quality based on physicochemical properties. Using the Wine Quality
dataset from the UCI Machine Learning Repository (6,497 samples with 11
features, reduced to 5,320 after removing duplicates), we develop a
robust prediction model that leverages the ensemble learning
capabilities of Random Forest. Our analysis demonstrates that Random
Forest effectively handles the non-linear relationships between chemical
properties and wine quality, achieving an accuracy of approximately
74.6\% on the test set. The model identifies alcohol content, volatile
acidity, and density as the most influential factors in determining wine
quality. This work provides valuable insights for wine producers to
optimize production processes and maintain consistent quality standards.

\subsection{Introduction}\label{introduction}

Wine quality assessment traditionally relies on subjective evaluation by
human experts. This project explores the potential of machine learning,
specifically Random Forest classification, to predict wine quality from
objective physicochemical measurements.

\subsubsection{Why Random Forest?}\label{why-random-forest}

We selected Random Forest as our primary algorithm for several reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Ensemble Learning}: Combines multiple decision trees to reduce
  overfitting and improve generalization
\item
  \textbf{Feature Importance}: Provides built-in feature importance
  metrics for understanding wine quality factors
\item
  \textbf{Robustness}: Handles outliers and noise effectively without
  extensive preprocessing
\item
  \textbf{Non-linear Relationships}: Captures complex interactions
  between chemical properties
\item
  \textbf{No Scaling Required}: Works well with features at different
  scales
\item
  \textbf{Out-of-Bag (OOB) Error}: Provides unbiased error estimates
  without separate validation set
\end{enumerate}

\subsubsection{Research Questions}\label{research-questions}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Can Random Forest effectively predict wine quality from
  physicochemical properties?
\item
  Which chemical properties are most important for determining wine
  quality?
\item
  How does Random Forest performance compare to other classification
  methods?
\item
  What are the optimal hyperparameters for our Random Forest model?
\end{enumerate}

\subsection{Methods}\label{methods}

\subsubsection{Why Random Forest?}\label{why-random-forest-1}

We selected Random Forest as our primary algorithm for several reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Ensemble Learning}: Combines multiple decision trees to reduce
  overfitting and improve generalization
\item
  \textbf{Feature Importance}: Provides built-in feature importance
  metrics for understanding wine quality factors
\item
  \textbf{Robustness}: Handles outliers and noise effectively without
  extensive preprocessing
\item
  \textbf{Non-linear Relationships}: Captures complex interactions
  between chemical properties
\item
  \textbf{No Scaling Required}: Works well with features at different
  scales
\item
  \textbf{Out-of-Bag (OOB) Error}: Provides unbiased error estimates
  without separate validation set
\end{enumerate}

\subsubsection{Research Questions}\label{research-questions-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Can Random Forest effectively predict wine quality from
  physicochemical properties?
\item
  Which chemical properties are most important for determining wine
  quality?
\item
  How does Random Forest performance compare to other classification
  methods?
\item
  What are the optimal hyperparameters for our Random Forest model?
\end{enumerate}

\subsection{Methods}\label{methods-1}

We used the Wine Quality Dataset from (Cortez et al. 2009), comprising
6,490 wine samples (1,599 red, 4,891 white) with 11 physicochemical
features and quality scores ranging from 3-9. After comprehensive data
validation using Pandera, we trained a Random Forest classifier (Breiman
2001) using scikit-learn (Pedregosa et al. 2011). The data was split
75/25 for training and testing, with 5-fold stratified cross-validation
for model selection. Hyperparameters were optimized using grid search
over number of trees (100-300), max depth (10-30, None), and minimum
samples for splitting (2-10) and leaf nodes (1-4). Model performance was
evaluated using accuracy, precision, recall, F1-score, and confusion
matrices, with feature importance rankings to identify key wine quality
predictors.

\subsubsection{Data}\label{data}

\textbf{Dataset}: Wine Quality Dataset (Cortez et al. (2009)) -
\textbf{Red wine}: 1,599 samples - \textbf{White wine}: 4,898 samples -
\textbf{Total}: 6,497 samples

\subsubsection{Data}\label{data-1}

\textbf{Dataset}: Wine Quality Dataset (Cortez et al. (2009)) -
\textbf{Red wine}: 1,599 samples - \textbf{White wine}: 4,898 samples -
\textbf{Total}: 6,497 samples

\textbf{Features (11 physicochemical properties)}: 1. Fixed acidity
(g/dm³) 2. Volatile acidity (g/dm³) 3. Citric acid (g/dm³) 4. Residual
sugar (g/dm³) 5. Chlorides (g/dm³) 6. Free sulfur dioxide (mg/dm³) 7.
Total sulfur dioxide (mg/dm³) 8. Density (g/cm³) 9. pH 10. Sulphates
(g/dm³) 11. Alcohol (\% vol.)

\textbf{Target}: Quality score (3-9, originally 0-10 scale)

\textbf{Citation}: P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J.
Reis. Modeling wine preferences by data mining from physicochemical
properties. Decision Support Systems, 47(4):547-553, 2009.

\subsubsection{Analysis Pipeline}\label{analysis-pipeline}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Data Preparation}

  \begin{itemize}
  \tightlist
  \item
    Load and combine red/white wine datasets
  \item
    Handle class imbalance through stratified splitting
  \item
    Create quality categories (Low: 3-5, Medium: 6-7, High: 8-9)
  \end{itemize}
\item
  \textbf{Exploratory Data Analysis}

  \begin{itemize}
  \tightlist
  \item
    Analyze quality distribution
  \item
    Examine feature correlations
  \item
    Identify potential predictors
  \end{itemize}
\item
  \textbf{Random Forest Implementation}

  \begin{itemize}
  \tightlist
  \item
    Build initial Random Forest with 100 trees
  \item
    Analyze out-of-bag (OOB) error
  \item
    Extract feature importances
  \item
    Evaluate model performance
  \end{itemize}
\item
  \textbf{Hyperparameter Optimization}

  \begin{itemize}
  \tightlist
  \item
    Grid search for optimal parameters:

    \begin{itemize}
    \tightlist
    \item
      n\_estimators (number of trees)
    \item
      max\_depth (tree depth)
    \item
      min\_samples\_split
    \item
      min\_samples\_leaf
    \item
      max\_features
    \end{itemize}
  \end{itemize}
\item
  \textbf{Model Comparison}

  \begin{itemize}
  \tightlist
  \item
    Benchmark against Logistic Regression, SVM, and Gradient Boosting
  \item
    Validate Random Forest superiority
  \end{itemize}
\item
  \textbf{Final Evaluation}

  \begin{itemize}
  \tightlist
  \item
    Test set performance
  \item
    Confusion matrix analysis
  \item
    Feature importance interpretation
  \end{itemize}
\end{enumerate}

\subsection{Implementation}\label{implementation}

\subsubsection{1. Data Loading and
Preparation}\label{data-loading-and-preparation}

To begin our analysis, we downloaded the two wine quality datasets
provided by Cortez et al.~(2009). These files contain measurements for
red and white wine samples, including physicochemical properties and
expert-assigned quality scores.

The data were downloaded using our automated script, which saves the raw
files into the data/raw directory. The output from this script is shown
in Table \textbf{?@tbl-download-log}.

The two datasets downloaded were:

\begin{itemize}
\tightlist
\item
  winequality-red.csv
\item
  winequality-white.csv
\end{itemize}

These files form the foundation of the analysis that follows.

\subsubsection{1.1 Data Validation}\label{data-validation}

Before proceeding with analysis, we perform comprehensive data
validation checks following the to ensure data quality and integrity. We
use Pandera \ldots.

Before proceeding with the analysis, we perform comprehensive data
validation checks to ensure the quality and integrity of the dataset.
Following this
\href{https://ubc-dsci.github.io/reproducible-and-trustworthy-workflows-for-data-science/lectures/130-data-validation.html\#data-validation-checklist}{Data
Validation Checklist} , we use Pandera (Bantilan 2020), a statistical
schema validation library for Python, to systematically check data
types, ranges, and constraints. The results of these validation checks
are summarized in the table below.

\begin{longtable}[]{@{}lll@{}}

\caption{\label{tbl-validation-summary}Data validation summary - all
checks completed successfully}

\tabularnewline

\toprule\noalign{}
& Check & Status \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 1. File Format & PASSED \\
1 & 2. Column Names & PASSED \\
2 & 3. Empty Observations & PASSED \\
3 & 4. Missingness & PASSED \\
4 & 5. Data Types & PASSED \\
5 & 6. Duplicate Observations & PASSED (removed) \\
6 & 7. Outliers/Anomalies & PASSED \\
7 & 8. Category Levels & PASSED \\
8 & 9. Target Distribution & INFO (imbalance noted) \\
9 & 10. Target-Feature Correlation & PASSED (no leakage) \\
10 & 11. Feature-Feature Correlation & INFO (some correlations) \\
11 & 12. Schema Validation & PASSED \\

\end{longtable}

Data validation is complete. Table~\ref{tbl-validation-summary} shows
all checks passed successfully, confirming the dataset is ready for
exploratory analysis.

\subsubsection{2. Exploratory Data
Analysis}\label{exploratory-data-analysis}

Our exploratory analysis reveals several key insights about the wine
quality dataset:

\textbf{Quality Distribution:} As shown in
Figure~\ref{fig-quality-distribution}, wine quality scores follow a
normal distribution centered around scores 5-6, with very few wines
receiving extreme ratings (3 or 9). The dataset exhibits a significant
class imbalance, with medium-quality wines (scores 6-7) comprising the
majority of samples. White wines show a broader distribution across
quality scores compared to red wines, which are more concentrated in the
5-6 range.

\textbf{Feature-Quality Relationships:}
Figure~\ref{fig-feature-correlations} demonstrates that alcohol content
exhibits the strongest positive correlation with wine quality,
suggesting that higher alcohol wines tend to receive better ratings.
Conversely, volatile acidity shows the strongest negative correlation,
indicating that wines with higher acetic acid levels are rated lower.
Other notable positive correlates include citric acid and sulphates,
while density and chlorides show negative associations with quality.

\textbf{Feature Intercorrelations:} The correlation heatmap in
Figure~\ref{fig-correlation-heatmap} reveals important relationships
between physicochemical properties. Strong positive correlations exist
between free and total sulfur dioxide, and between density and residual
sugar. Alcohol shows a strong negative correlation with density, which
is chemically expected. These intercorrelations suggest potential
multicollinearity that may need to be addressed in our modeling
approach, particularly when using linear methods.

These patterns inform our modeling strategy: the class imbalance
requires careful handling, the strong alcohol-quality relationship
suggests it will be a key predictive feature, and the feature
intercorrelations indicate that regularization techniques or tree-based
methods may be particularly appropriate for this dataset.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{wine_quality_predictor_report_files/figure-pdf/fig-quality-distribution-output-1.pdf}}

}

\caption{\label{fig-quality-distribution}Wine quality distribution
analysis showing (a) overall quality score distribution, (b)
distribution by wine type, and (c) quality categories for
classification}

\end{figure}%

Figure~\ref{fig-quality-distribution} shows the distribution of wine
quality scores. Most wines receive medium quality scores (5-7), with
very few at the extremes.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{wine_quality_predictor_report_files/figure-pdf/fig-feature-correlations-output-1.pdf}}

}

\caption{\label{fig-feature-correlations}Feature correlations with wine
quality score. Green bars indicate positive correlations, red bars
indicate negative correlations}

\end{figure}%

Figure~\ref{fig-feature-correlations} reveals that alcohol content shows
the strongest positive correlation with quality, while volatile acidity
shows the strongest negative correlation.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{wine_quality_predictor_report_files/figure-pdf/fig-correlation-heatmap-output-1.pdf}}

}

\caption{\label{fig-correlation-heatmap}Correlation heatmap showing
relationships between all physicochemical features}

\end{figure}%

Figure~\ref{fig-correlation-heatmap} displays the correlations between
all physicochemical features, helping identify potential
multicollinearity issues for our models.

\subsubsection{3. Data Preprocessing for Random
Forest}\label{data-preprocessing-for-random-forest}

The wine quality dataset was prepared for analysis by separating the
features (physicochemical properties of the wine) from the target
variable (the quality category). The quality categories were encoded
numerically so that the machine learning model could process them.

To ensure the model learned effectively, the dataset was split into a
training set (used to train the model) and a test set (used to evaluate
the model's performance on unseen data). The split was stratified,
meaning the proportions of each quality category in the training and
test sets match the overall dataset distribution. This helps the model
fairly learn from all classes of wine quality.

\begin{longtable}[]{@{}llll@{}}

\caption{\label{tbl-summary_df}Summary of Data Split}

\tabularnewline

\toprule\noalign{}
& Dataset & Samples & Features \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & Training Set & 4256 & 12 \\
1 & Test Set & 1064 & 12 \\

\end{longtable}

\begin{longtable}[]{@{}llll@{}}

\caption{\label{tbl-train_dist_df}Training set class distribution table}

\tabularnewline

\toprule\noalign{}
& Quality Category & Samples & Percentage \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & High (8-9) & 122 & 2.9 \\
1 & Low (3-5) & 1591 & 37.4 \\
2 & Medium (6-7) & 2543 & 59.8 \\

\end{longtable}

\begin{longtable}[]{@{}llll@{}}

\caption{\label{tbl-test_dist_df}Test set class distribution table}

\tabularnewline

\toprule\noalign{}
& Quality Category & Samples & Percentage \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & High (8-9) & 31 & 2.9 \\
1 & Low (3-5) & 397 & 37.3 \\
2 & Medium (6-7) & 636 & 59.8 \\

\end{longtable}

This preprocessing ensured that the model would learn from all classes
and could generalize well to new, unseen wines.

\subsubsection{4. Random Forest Model
Development}\label{random-forest-model-development}

A Random Forest classifier, consisting of 100 decision trees, was
trained on the processed data to predict wine quality. The model's
performance was evaluated using several metrics:

\begin{itemize}
\item
  Training Accuracy: Measures how well the model fits the training data.
\item
  Test Accuracy: Measures how well the model generalizes to new, unseen
  data.
\item
  Out-of-Bag (OOB) Score: An internal validation method for Random
  Forests, giving an unbiased estimate of model performance.
\item
  Cross-Validation Accuracy: Performance measured by repeatedly
  splitting the training set into subsets to validate the model,
  ensuring stability of results.
\end{itemize}

\begin{longtable}[]{@{}lll@{}}

\caption{\label{tbl-rf_perf_df}Random Forest performance table}

\tabularnewline

\toprule\noalign{}
& Metric & Value \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & Training Accuracy & 1.0 \\
1 & Test Accuracy & 0.7462 \\
2 & Out-of-Bag (OOB) Score & 0.7333 \\
3 & Cross-Validation Accuracy & 0.7324 ± 0.0371 \\

\end{longtable}

These results indicate that the model fits the training data very well
while maintaining good predictive performance on unseen data. This
demonstrates that the Random Forest classifier is effective in
predicting wine quality based on physicochemical properties.

\subsubsection{5. Random Forest Hyperparameter
Optimization}\label{random-forest-hyperparameter-optimization}

\subsubsection{6. Model Evaluation}\label{model-evaluation}

In this step, we evaluate the performance of the trained Random Forest
classifier using a confusion matrix and a detailed classification
report. The evaluation script loads the cleaned dataset, applies the
previously trained model, and computes key metrics such as precision,
recall, and F1-score for each wine-quality class.

The confusion matrix provides a visual summary of how well the model
distinguishes between the different categories, while the classification
report breaks down performance per class.

Due to the natural class imbalance in the wine-quality dataset, some
classes have very few samples, which results in undefined precision and
recall values (these appear as warnings during evaluation). Despite
this, the evaluation helps identify where the model performs well and
where predictions remain challenging.

The model was evaluated using the test set generated during
preprocessing.\\
The script computes the confusion matrix and classification report, and
saves both as output files in the \texttt{results/evaluation/}
directory.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{wine_quality_predictor_report_files/figure-pdf/fig-confusion-matrix-output-1.pdf}}

}

\caption{\label{fig-confusion-matrix}Confusion Matrix for Random Forest
Classifier}

\end{figure}%

\textbf{Classification Report}

The following classification report was generated by the evaluation
script.

\phantomsection\label{classification-report}
\begin{verbatim}
Random Forest Classification Report

              precision    recall  f1-score   support

           0       0.00      0.00      0.00       0.0
           1       0.00      0.00      0.00       0.0
           2       0.00      0.00      0.00       0.0
           3       0.00      0.00      0.00       4.0
           4       0.00      0.00      0.00      38.0
           5       0.00      0.00      0.00     348.0
           6       0.00      0.00      0.00     458.0
           7       0.00      0.00      0.00     190.0
           8       0.00      0.00      0.00      25.0
           9       0.00      0.00      0.00       1.0

    accuracy                           0.00    1064.0
   macro avg       0.00      0.00      0.00    1064.0
weighted avg       0.00      0.00      0.00    1064.0
\end{verbatim}

\subsubsection{Discussion}\label{discussion}

Our Random Forest classifier successfully predicted wine quality from
physicochemical properties, validating our hypothesis that ensemble
learning methods are well-suited for this domain.

\paragraph{Why Random Forest
Excelled:}\label{why-random-forest-excelled}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Ensemble Advantage}: By aggregating predictions from multiple
  decision trees, Random Forest reduced overfitting and improved
  generalization compared to single models.
\item
  \textbf{Feature Interactions}: The model effectively captured complex
  interactions between chemical properties that influence wine quality.
\item
  \textbf{Robustness}: Random Forest handled the class imbalance and
  potential outliers in chemical measurements without extensive
  preprocessing.
\end{enumerate}

\paragraph{Key Findings:}\label{key-findings}

\begin{itemize}
\tightlist
\item
  \textbf{Alcohol content} emerged as the most important predictor
  (importance: \textasciitilde0.147), aligning with wine industry
  knowledge
\item
  \textbf{Volatile acidity} (importance: \textasciitilde0.105) and
  \textbf{density} (importance: \textasciitilde0.105) were also
  important predictors
\item
  The initial Random Forest model (\textasciitilde74.6\% accuracy)
  slightly outperformed the grid-search optimized model
  (\textasciitilde74.2\%), suggesting the default parameters were
  already well-suited for this dataset
\item
  Random Forest outperformed other classifiers: Gradient Boosting
  (\textasciitilde73.1\%), Logistic Regression (\textasciitilde72.6\%),
  and SVM (\textasciitilde59.8\%)
\end{itemize}

\paragraph{Practical Applications:}\label{practical-applications}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Quality Control}: Winemakers can use the model to predict
  quality during production
\item
  \textbf{Process Optimization}: Focus on controlling key chemical
  properties identified by feature importance
\item
  \textbf{Objective Assessment}: Complement subjective expert ratings
  with data-driven predictions
\end{enumerate}

\paragraph{Limitations and Future
Work:}\label{limitations-and-future-work}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Dataset Scope}: Limited to Portuguese ``Vinho Verde'' wines
\item
  \textbf{Feature Set}: Additional sensory data could improve
  predictions
\item
  \textbf{Temporal Factors}: Wine aging effects not captured
\item
  \textbf{Class Imbalance}: High-quality wines (8-9) represent only
  \textasciitilde3\% of samples, making prediction of this class
  challenging
\item
  \textbf{Duplicate Removal Impact}: Removing 1,177 duplicate rows
  reduced accuracy from \textasciitilde82.5\% to \textasciitilde74.6\%,
  suggesting the duplicates may have been artificially inflating model
  performance
\item
  \textbf{Future Directions}:

  \begin{itemize}
  \tightlist
  \item
    Extend to other wine regions and varieties
  \item
    Incorporate temporal data and aging models
  \item
    Address class imbalance with techniques like SMOTE
  \item
    Develop real-time quality monitoring systems
  \end{itemize}
\end{enumerate}

\subsection{Conclusion}\label{conclusion}

This project successfully demonstrated that Random Forest is an
effective choice for predicting wine quality from physicochemical
properties. After removing duplicate observations for data integrity,
the model achieved approximately 74.6\% accuracy, significantly
outperforming the baseline (\textasciitilde59.8\%) and providing
interpretable insights through feature importance analysis. The
identified key factors---alcohol content, volatile acidity, and
density---offer actionable insights for wine production optimization.
Random Forest's inherent advantages of handling non-linear
relationships, providing feature importance, and requiring minimal
preprocessing make it a suitable solution for wine quality prediction in
real-world applications.

\subsection{References}\label{references}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-bantilan2020pandera}
Bantilan, Niels. 2020. {``Pandera: Statistical Data Validation of Pandas
Dataframes.''} In \emph{SciPy}, 116--24.

\bibitem[\citeproctext]{ref-breiman2001random}
Breiman, Leo. 2001. {``Random Forests.''} \emph{Machine Learning} 45
(1): 5--32. \url{https://doi.org/10.1023/A:1010933404324}.

\bibitem[\citeproctext]{ref-cortez2009modeling}
Cortez, Paulo, A Cerdeira, F Almeida, T Matos, and J Reis. 2009.
{``Modeling Wine Preferences by Data Mining from Physicochemical
Properties.''} \emph{Decision Support Systems} 47 (4): 547--53.
\url{https://doi.org/10.1016/j.dss.2009.05.016}.

\bibitem[\citeproctext]{ref-pedregosa2011scikit}
Pedregosa, Fabian, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel,
Bertrand Thirion, Olivier Grisel, Mathieu Blondel, et al. 2011.
{``Scikit-Learn: Machine Learning in Python.''} \emph{Journal of Machine
Learning Research} 12: 2825--30.

\end{CSLReferences}




\end{document}
