---
title: Predicting Wine Quality using Random Forest Classifier
author: "Junli Liu, Purity Jangaya, Luis Alonso Alvarez & Jimmy Wang"
date: "2025/11/20"
jupyter: python3

format:
  html:
    toc: true
    toc-depth: 4
    #output-file: index.html
  pdf:
    toc: true
    toc-depth: 3

bibliography: references.bib

execute:
  echo: false
  warning: false
  python:
    eval: true
    enabled: true

editor: source
---

## Summary

This project implements a Random Forest classifier to predict wine quality based on physicochemical properties. Using the Wine Quality dataset from the UCI Machine Learning Repository (6,497 samples with 11 features, reduced to 5,320 after removing duplicates), we develop a robust prediction model that leverages the ensemble learning capabilities of Random Forest. Our analysis demonstrates that Random Forest effectively handles the non-linear relationships between chemical properties and wine quality, achieving an accuracy of approximately 74.6% on the test set. The model identifies alcohol content, volatile acidity, and density as the most influential factors in determining wine quality. This work provides valuable insights for wine producers to optimize production processes and maintain consistent quality standards.

## Introduction

Wine quality assessment traditionally relies on subjective evaluation by human experts. This project explores the potential of machine learning, specifically Random Forest classification, to predict wine quality from objective physicochemical measurements.

### Why Random Forest?

We selected Random Forest as our primary algorithm for several reasons:

1. **Ensemble Learning**: Combines multiple decision trees to reduce overfitting and improve generalization
2. **Feature Importance**: Provides built-in feature importance metrics for understanding wine quality factors
3. **Robustness**: Handles outliers and noise effectively without extensive preprocessing
4. **Non-linear Relationships**: Captures complex interactions between chemical properties
5. **No Scaling Required**: Works well with features at different scales
6. **Out-of-Bag (OOB) Error**: Provides unbiased error estimates without separate validation set

### Research Questions

1. Can Random Forest effectively predict wine quality from physicochemical properties?
2. Which chemical properties are most important for determining wine quality?
3. How does Random Forest performance compare to other classification methods?
4. What are the optimal hyperparameters for our Random Forest model?

## Methods

### Why Random Forest?

We selected Random Forest as our primary algorithm for several reasons:

1. **Ensemble Learning**: Combines multiple decision trees to reduce overfitting and improve generalization
2. **Feature Importance**: Provides built-in feature importance metrics for understanding wine quality factors
3. **Robustness**: Handles outliers and noise effectively without extensive preprocessing
4. **Non-linear Relationships**: Captures complex interactions between chemical properties
5. **No Scaling Required**: Works well with features at different scales
6. **Out-of-Bag (OOB) Error**: Provides unbiased error estimates without separate validation set

### Research Questions

1. Can Random Forest effectively predict wine quality from physicochemical properties?
2. Which chemical properties are most important for determining wine quality?
3. How does Random Forest performance compare to other classification methods?
4. What are the optimal hyperparameters for our Random Forest model?

## Methods

We used the Wine Quality Dataset from [@cortez2009modeling], comprising 6,490 wine samples (1,599 red, 4,891 white) with 11 physicochemical features and quality scores ranging from 3-9. After comprehensive data validation using Pandera, we trained a Random Forest classifier [@breiman2001random] using scikit-learn [@pedregosa2011scikit]. The data was split 75/25 for training and testing, with 5-fold stratified cross-validation for model selection. Hyperparameters were optimized using grid search over number of trees (100-300), max depth (10-30, None), and minimum samples for splitting (2-10) and leaf nodes (1-4). Model performance was evaluated using accuracy, precision, recall, F1-score, and confusion matrices, with feature importance rankings to identify key wine quality predictors.

### Data

**Dataset**: Wine Quality Dataset (@cortez2009modeling)
- **Red wine**: 1,599 samples
- **White wine**: 4,898 samples
- **Total**: 6,497 samples

### Data

**Dataset**: Wine Quality Dataset (@cortez2009modeling)
- **Red wine**: 1,599 samples
- **White wine**: 4,898 samples
- **Total**: 6,497 samples

**Features (11 physicochemical properties)**:
1. Fixed acidity (g/dm³)
2. Volatile acidity (g/dm³)
3. Citric acid (g/dm³)
4. Residual sugar (g/dm³)
5. Chlorides (g/dm³)
6. Free sulfur dioxide (mg/dm³)
7. Total sulfur dioxide (mg/dm³)
8. Density (g/cm³)
9. pH
10. Sulphates (g/dm³)
11. Alcohol (% vol.)

**Target**: Quality score (3-9, originally 0-10 scale)

**Citation**: P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. Decision Support Systems, 47(4):547-553, 2009.

### Analysis Pipeline

1. **Data Preparation**
   - Load and combine red/white wine datasets
   - Handle class imbalance through stratified splitting
   - Create quality categories (Low: 3-5, Medium: 6-7, High: 8-9)

2. **Exploratory Data Analysis**
   - Analyze quality distribution
   - Examine feature correlations
   - Identify potential predictors

3. **Random Forest Implementation**
   - Build initial Random Forest with 100 trees
   - Analyze out-of-bag (OOB) error
   - Extract feature importances
   - Evaluate model performance

4. **Hyperparameter Optimization**
   - Grid search for optimal parameters:
     - n_estimators (number of trees)
     - max_depth (tree depth)
     - min_samples_split
     - min_samples_leaf
     - max_features

5. **Model Comparison**
   - Benchmark against Logistic Regression, SVM, and Gradient Boosting
   - Validate Random Forest superiority

6. **Final Evaluation**
   - Test set performance
   - Confusion matrix analysis
   - Feature importance interpretation

## Implementation

```{python}
#| echo: false
#| output: false

# Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import (
    classification_report, confusion_matrix, accuracy_score,
    precision_recall_fscore_support, roc_auc_score, roc_curve
)
import pandera as pa
from pandera import Column, Check, DataFrameSchema
import warnings
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
np.random.seed(42)

# Set plotting style
sns.set_theme(style="whitegrid")
sns.set_palette("husl")

print("Libraries loaded successfully!")
```

### 1. Data Loading and Preparation

To begin our analysis, we downloaded the two wine quality datasets provided by Cortez et al. (2009). These files contain measurements for red and white wine samples, including physicochemical properties and expert-assigned quality scores.

The data were downloaded using our automated script, which saves the raw files into the data/raw directory.
The output from this script is shown in Table @tbl-download-log.

The two datasets downloaded were:

- winequality-red.csv
- winequality-white.csv

These files form the foundation of the analysis that follows.

```{python}
#| echo: false
#| output: false

# Load datasets
red_wine = pd.read_csv('../data/raw/winequality-red.csv', sep=';')
white_wine = pd.read_csv('../data/raw/winequality-white.csv', sep=';')

# Add wine type indicator
red_wine['wine_type'] = 0  # 0 for red
white_wine['wine_type'] = 1  # 1 for white

# Combine datasets
wine_data = pd.concat([red_wine, white_wine], ignore_index=True)

print("Dataset Overview:")
print(f"Total samples: {len(wine_data)}")
print(f"Red wine: {len(red_wine)} samples ({len(red_wine)/len(wine_data)*100:.1f}%)")
print(f"White wine: {len(white_wine)} samples ({len(white_wine)/len(wine_data)*100:.1f}%)")
print(f"Features: {wine_data.shape[1] - 1}")
print(f"\nQuality distribution:")
print(wine_data['quality'].value_counts().sort_index())
```


```{python}
#| echo: false
#| output: false

# Check for missing values and data types
print("Data Quality Check:")
print(f"Missing values: {wine_data.isnull().sum().sum()}")
print(f"\nData types:")
print(wine_data.dtypes)
print(f"\nBasic statistics:")
wine_data.describe()
```

### 1.1 Data Validation

Before proceeding with analysis, we perform comprehensive data validation checks following the  to ensure data quality and integrity. We use Pandera ....

Before proceeding with the analysis, we perform comprehensive data validation checks to ensure the quality and integrity of the dataset. Following this [Data Validation Checklist](https://ubc-dsci.github.io/reproducible-and-trustworthy-workflows-for-data-science/lectures/130-data-validation.html#data-validation-checklist)
, we use Pandera [@bantilan2020pandera], a statistical schema validation library for Python, to systematically check data types, ranges, and constraints. The results of these validation checks are summarized in the table below.

```{python}
#| echo: false
#| output: false

# Data Validation using Pandera
# Define expected column names for raw wine data
EXPECTED_COLUMNS = [
    'fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',
    'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',
    'pH', 'sulphates', 'alcohol', 'quality'
]

# Define valid quality score range
VALID_QUALITY_RANGE = (3, 9)

# Define reasonable ranges for physicochemical properties based on domain knowledge
FEATURE_RANGES = {
    'fixed acidity': (0, 20),        # g/dm³ - typical wine range
    'volatile acidity': (0, 2),       # g/dm³ - acetic acid
    'citric acid': (0, 2),            # g/dm³
    'residual sugar': (0, 100),       # g/dm³
    'chlorides': (0, 1),              # g/dm³
    'free sulfur dioxide': (0, 300),  # mg/dm³
    'total sulfur dioxide': (0, 500), # mg/dm³
    'density': (0.9, 1.1),            # g/cm³ - close to water
    'pH': (2, 5),                     # typical wine pH range
    'sulphates': (0, 3),              # g/dm³
    'alcohol': (5, 20)                # % vol.
}

print("Data Validation Configuration:")
print(f"Expected columns: {len(EXPECTED_COLUMNS)}")
print(f"Valid quality range: {VALID_QUALITY_RANGE}")
print("Feature ranges defined for anomaly detection.")
```


```{python}
#| echo: false
#| output: false

# Validation Check 1 & 2: File Format and Column Names
print("="*60)
print("VALIDATION CHECK 1 & 2: File Format and Column Names")
print("="*60)

def validate_column_names(df, expected_cols, dataset_name):
    """Validate that all expected columns are present."""
    actual_cols = set(df.columns)
    expected_set = set(expected_cols)
    
    missing = expected_set - actual_cols
    extra = actual_cols - expected_set
    
    if missing:
        raise ValueError(f"{dataset_name}: Missing columns: {missing}")
    if extra:
        print(f"  WARNING: {dataset_name} has extra columns: {extra}")
    
    print(f"  {dataset_name}: All {len(expected_cols)} expected columns present")
    return True

# Validate red wine
validate_column_names(red_wine.drop('wine_type', axis=1), EXPECTED_COLUMNS, "Red wine")

# Validate white wine
validate_column_names(white_wine.drop('wine_type', axis=1), EXPECTED_COLUMNS, "White wine")

# Validate combined dataset
validate_column_names(wine_data.drop(['wine_type', 'quality_category'], axis=1, errors='ignore'), 
                     EXPECTED_COLUMNS, "Combined dataset")

print("\n[PASSED] File format and all column names validation")
```

```{python}
#| echo: false
#| output: false

# Validation Check 3: Empty Observations
print("="*60)
print("VALIDATION CHECK 3: Empty Observations")
print("="*60)

def check_empty_observations(df, dataset_name):
    """Check for completely empty rows."""
    empty_rows = df.isnull().all(axis=1).sum()
    if empty_rows > 0:
        raise ValueError(f"{dataset_name}: Found {empty_rows} completely empty rows")
    print(f"  {dataset_name}: No completely empty observations found")
    return True

check_empty_observations(red_wine, "Red wine")
check_empty_observations(white_wine, "White wine")
check_empty_observations(wine_data, "Combined dataset")

print("\n[PASSED] No empty observations found")
```

```{python}
#| echo: false
#| output: false

# Validation Check 4: Missingness
print("="*60)
print("VALIDATION CHECK 4: Missingness")
print("="*60)

MISSING_THRESHOLD = 0.05  # Allow up to 5% missing values per column

def check_missingness(df, threshold, dataset_name):
    """Check that missing values do not exceed threshold."""
    missing_pct = df.isnull().mean()
    violations = missing_pct[missing_pct > threshold]
    
    if len(violations) > 0:
        raise ValueError(f"{dataset_name}: Columns exceed {threshold*100}% missing threshold:\n{violations}")
    
    total_missing = df.isnull().sum().sum()
    print(f"  {dataset_name}: Total missing values = {total_missing}")
    print(f"  {dataset_name}: Max missing % per column = {missing_pct.max()*100:.2f}%")
    return True

check_missingness(red_wine, MISSING_THRESHOLD, "Red wine")
check_missingness(white_wine, MISSING_THRESHOLD, "White wine")
check_missingness(wine_data, MISSING_THRESHOLD, "Combined dataset")

print(f"\n[PASSED] All columns below {MISSING_THRESHOLD*100}% missing threshold")
```

```{python}
#| echo: false
#| output: false

# Validation Check 5: Data Types
print("="*60)
print("VALIDATION CHECK 5: Data Types")
print("="*60)

EXPECTED_DTYPES = {
    'fixed acidity': 'float64',
    'volatile acidity': 'float64',
    'citric acid': 'float64',
    'residual sugar': 'float64',
    'chlorides': 'float64',
    'free sulfur dioxide': 'float64',
    'total sulfur dioxide': 'float64',
    'density': 'float64',
    'pH': 'float64',
    'sulphates': 'float64',
    'alcohol': 'float64',
    'quality': 'int64'
}

def check_data_types(df, expected_dtypes, dataset_name):
    """Validate that columns have expected data types."""
    type_errors = []
    for col, expected_type in expected_dtypes.items():
        if col in df.columns:
            actual_type = str(df[col].dtype)
            if actual_type != expected_type:
                type_errors.append(f"  {col}: expected {expected_type}, got {actual_type}")
    
    if type_errors:
        raise ValueError(f"{dataset_name}: Data type mismatches:\n" + "\n".join(type_errors))
    
    print(f"  {dataset_name}: All columns have correct data types")
    return True

check_data_types(wine_data, EXPECTED_DTYPES, "Combined dataset")

print("\n[PASSED] Data types validation")
```

```{python}
#| echo: false
#| output: false

# Validation Check 6: Duplicate Observations
print("="*60)
print("VALIDATION CHECK 6: Duplicate Observations")
print("="*60)

def check_duplicates(df, dataset_name):
    """Check for and report duplicate rows."""
    duplicates = df.duplicated().sum()
    duplicate_pct = duplicates / len(df) * 100
    
    print(f"  {dataset_name}: {duplicates} duplicate rows ({duplicate_pct:.2f}%)")
    
    return duplicates

dup_red = check_duplicates(red_wine, "Red wine")
dup_white = check_duplicates(white_wine, "White wine")
dup_combined = check_duplicates(wine_data, "Combined dataset")

# Remove duplicates from combined dataset
print(f"\nRemoving {dup_combined} duplicate rows from combined dataset...")
wine_data = wine_data.drop_duplicates().reset_index(drop=True)
print(f"Dataset size after removing duplicates: {len(wine_data)} samples")

# Verify no duplicates remain
dup_after = wine_data.duplicated().sum()
print(f"Duplicates remaining: {dup_after}")

print("\n[PASSED] Duplicate observations removed")
```

```{python}
#| echo: false
#| output: false

# Validation Check 7: No Outliers neither Anamalous Values
print("="*60)
print("VALIDATION CHECK 7: NO OUTLIERS OR ANAMALOUS VALUES")
print("="*60)

def check_value_ranges(df, feature_ranges, dataset_name):
    """Check for values out of domain ranges."""
    outliers={}

    for column, (min_val, max_val) in feature_ranges.items():
        below_min = (df[column] < min_val).sum()
        above_max = (df[column] > max_val).sum()

        if below_min > 0 or above_max > 0:
            outliers[column] = {
                'below_min': below_min,
                'above_max': above_max,
                'expected_range': (min_val, max_val),
                'actual_range': (df[column].min(), df[column].max())
            }
        
    if outliers:
        print(f"  {dataset_name}: Found values outside expected ranges:")
        for column, info in outliers.items():
            print(f"    {column}: expected [{info['expected_range'][0]}, {info['expected_range'][1]}], "
                  f"got [{info['actual_range'][0]:.3f}, {info['actual_range'][1]:.3f}]")
            print(f"      - {info['below_min']} values below min, {info['above_max']} values above max")
    else:
        print(f"  {dataset_name}: All values within expected ranges")
    
    return outliers

outliers = check_value_ranges(wine_data, FEATURE_RANGES, "Combined dataset")

if not outliers:
    print("\n[PASSED] No anomalous values neither outliers detected")
else:
    print("\n[WARNING] Some values outside expected ranges - investigate before proceeding")
```

```{python}
#| echo: false
#| output: false

# Validation Check 8: Correct Category Levels (for categorical features)
print("="*60)
print("VALIDATION CHECK 8: CATEGORY LEVELS")
print("="*60)

def check_category_levels(df, column, expected_values, dataset_name):
    """Validate categorical column has expected values"""
    actual_values = set(df[column].unique())
    expected_set = set(expected_values)

    unexpected_set = actual_values - expected_set
    missing_set = expected_set - actual_values

    if unexpected_set:
        print(f"\n WARNING: {dataset_name} {column} has unexpected values: {unexpected_set}")
    if missing_set:
        print(f"\n  INFO: {dataset_name} {column} missing expected values: {missing_set}")

    print(f"\n {dataset_name} {column} has {len(actual_values)} unique values: {sorted(actual_values)}")
    return len(unexpected_set) == 0

# Check quality values (should be integers 3-9)
expected_quality = list(range(3, 10))  # 3, 4, 5, 6, 7, 8, 9
check_category_levels(wine_data, 'quality', expected_quality, "Combined dataset")

# Check wine_type values (should be 0 or 1)
expected_wine_type = [0, 1]
check_category_levels(wine_data, 'wine_type', expected_wine_type, "Combined dataset")

print("\n[PASSED] Category levels validation")
```

```{python}
#| echo: false
#| output: false

# Validation Check 9: Target/response variable follows expected distribution
print("="*60)
print("VALIDATION CHECK 9: Target Variable follows expected distribution")
print("="*60)

def check_target_distribution(df, target_col, dataset_name):
    """Analyze target variable distribution for imbalance."""
    distribution = df[target_col].value_counts().sort_index()
    proportions = df[target_col].value_counts(normalize=True).sort_index()
    
    print(f"\n  {dataset_name} - {target_col} distribution:")
    for val, count in distribution.items():
        print(f"    {val}: {count} ({proportions[val]*100:.1f}%)")
    
    # Check for severe imbalance (any class < 1%)
    min_proportion = proportions.min()
    max_proportion = proportions.max()
    imbalance_ratio = max_proportion / min_proportion
    
    print(f"\n  Class imbalance ratio: {imbalance_ratio:.1f}:1")
    
    if min_proportion < 0.01:
        print(f"  WARNING: Severe class imbalance detected (min class = {min_proportion*100:.2f}%)")
    elif min_proportion < 0.05:
        print(f"  WARNING: Moderate class imbalance detected (min class = {min_proportion*100:.2f}%)")
    else:
        print(f"  Class distribution is acceptable for modeling")
    
    return imbalance_ratio

imbalance = check_target_distribution(wine_data, 'quality', "Combined dataset")
```

```{python}
#| echo: false
#| output: false

# Validation Check 10 & 11: Feature Correlations (Target-Feature and Feature-Feature)
print("="*60)
print("VALIDATION CHECK 10 & 11: Feature Correlations")
print("="*60)

def check_correlations(df, target_col, feature_cols, dataset_name):
    """Check for suspicious correlations that might indicate data issues"""

    # Target-Feature correlations
    print(f"\n {dataset_name} - Target-Feature Correlations:")
    target_corr = df[feature_cols].corrwith(df[target_col]).abs().sort_values(ascending=False)

    # Flag if any feature has very high correlation with target(potential leakage)
    high_target_corr = target_corr[target_corr > 0.9]
    if len(high_target_corr) > 0:
        print(f"    WARNING: Features with suspiciously high target correlation (>0.9):")
        for feat, corr in high_target_corr.items():
            print(f"    {feat}: {corr:.3f}")
    else:
        print(f"    No suspiciously high target correlations detected")
    
    print(f"    Highest target correlation: {target_corr.index[0]} ({target_corr.iloc[0]:.3f})")
    print(f"  Lowest target correlation: {target_corr.index[-1]} ({target_corr.iloc[-1]:.3f})")

    # Feature-Feature correlations
    print(f"\n {dataset_name} - Feature-Feature Correlations:")
    corr_matrix = df[feature_cols].corr().abs()

    # Find highly correlated feature pairs (excluding diagonal)
    high_corr_pairs = []
    for i in range(len(feature_cols)):
        for j in range(i+1, len(feature_cols)):
            corr = corr_matrix.iloc[i, j]
            if corr > 0.8:
                high_corr_pairs.append((feature_cols[i], feature_cols[j], corr))
    
    if high_corr_pairs:
        print(f"    Highly correlated feature pairs (>0.8):")
        for f1, f2, corr in sorted(high_corr_pairs, key=lambda x: -x[2]):
            print(f"    {f1} <-> {f2}: {corr:.3f}")
    
    else:
        print(f"   No highly correlated features pairs (>0.8) detected")
    
    return target_corr, high_corr_pairs

feature_cols = [col for col in EXPECTED_COLUMNS if col != 'quality']
target_corr, high_corr = check_correlations(wine_data, 'quality', feature_cols, "Combined dataset")

print("\n[INFO] Correlation analysis complete - no data leakage detected")
```

```{python}
#| echo: false
#| output: false

# Comprehensive Pandera Schema Validation
print("="*60)
print("COMPREHENSIVE SCHEMA VALIDATION (Pandera)")

wine_schema = pa.DataFrameSchema({
    "fixed acidity": Column(float, Check.in_range(0, 20), nullable=False), 
    "volatile acidity": Column(float, Check.in_range(0, 2), nullable=False),
    "citric acid": Column(float, Check.in_range(0, 2), nullable=False),
    "residual sugar": Column(float, Check.in_range(0, 100), nullable=False),
    "chlorides": Column(float, Check.in_range(0, 1), nullable=False),
    "free sulfur dioxide": Column(float, Check.in_range(0, 300), nullable=False),
    "total sulfur dioxide": Column(float, Check.in_range(0, 500), nullable=False),
    "density": Column(float, Check.in_range(0.9, 1.1), nullable=False),
    "pH": Column(float, Check.in_range(2, 5), nullable=False),
    "sulphates": Column(float, Check.in_range(0, 3), nullable=False),
    "alcohol": Column(float, Check.in_range(5, 20), nullable=False),
    "quality": Column(int, Check.in_range(3, 9), nullable=False),
})

try:
    wine_schema.validate(wine_data[EXPECTED_COLUMNS])
    schema_status = "PASSED"
    print("\n[PASSED] Pandera schema validation successful!")
except pa.errors.SchemaError as e:
    schema_status = "FAILED"
    print(f"\n[FAILED] Schema validation error: {e}")
```
```{python}
#| echo: false
#| label: tbl-validation-summary
#| tbl-cap: "Data validation summary - all checks completed successfully"

validation_summary = pd.DataFrame({
    'Check': [
        '1. File Format',
        '2. Column Names',
        '3. Empty Observations',
        '4. Missingness',
        '5. Data Types',
        '6. Duplicate Observations',
        '7. Outliers/Anomalies',
        '8. Category Levels',
        '9. Target Distribution',
        '10. Target-Feature Correlation',
        '11. Feature-Feature Correlation',
        '12. Schema Validation'
    ],
    'Status': [
        'PASSED',
        'PASSED',
        'PASSED',
        'PASSED',
        'PASSED',
        'PASSED (removed)',
        'PASSED',
        'PASSED',
        'INFO (imbalance noted)',
        'PASSED (no leakage)',
        'INFO (some correlations)',
        schema_status
    ]
})

display(validation_summary)
```

Data validation is complete. @tbl-validation-summary shows all checks passed successfully, confirming the dataset is ready for exploratory analysis.


### 2. Exploratory Data Analysis

Our exploratory analysis reveals several key insights about the wine quality dataset:

**Quality Distribution:** As shown in @fig-quality-distribution, wine quality scores follow a normal distribution centered around scores 5-6, with very few wines receiving extreme ratings (3 or 9). The dataset exhibits a significant class imbalance, with medium-quality wines (scores 6-7) comprising the majority of samples. White wines show a broader distribution across quality scores compared to red wines, which are more concentrated in the 5-6 range.

**Feature-Quality Relationships:** @fig-feature-correlations demonstrates that alcohol content exhibits the strongest positive correlation with wine quality, suggesting that higher alcohol wines tend to receive better ratings. Conversely, volatile acidity shows the strongest negative correlation, indicating that wines with higher acetic acid levels are rated lower. Other notable positive correlates include citric acid and sulphates, while density and chlorides show negative associations with quality.

**Feature Intercorrelations:** The correlation heatmap in @fig-correlation-heatmap reveals important relationships between physicochemical properties. Strong positive correlations exist between free and total sulfur dioxide, and between density and residual sugar. Alcohol shows a strong negative correlation with density, which is chemically expected. These intercorrelations suggest potential multicollinearity that may need to be addressed in our modeling approach, particularly when using linear methods.

These patterns inform our modeling strategy: the class imbalance requires careful handling, the strong alcohol-quality relationship suggests it will be a key predictive feature, and the feature intercorrelations indicate that regularization techniques or tree-based methods may be particularly appropriate for this dataset.

```{python}
#| echo: false
#| label: fig-quality-distribution
#| fig-cap: "Wine quality distribution analysis showing (a) overall quality score distribution, (b) distribution by wine type, and (c) quality categories for classification"
#| fig-width: 15
#| fig-height: 5

# Quality distribution visualization
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# Overall distribution
wine_data['quality'].value_counts().sort_index().plot(kind='bar', ax=axes[0], color='steelblue')
axes[0].set_title('Overall Wine Quality Distribution', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Quality Score')
axes[0].set_ylabel('Count')
axes[0].grid(True, alpha=0.3)

# By wine type
quality_by_type = wine_data.groupby(['wine_type', 'quality']).size().unstack(fill_value=0)
quality_by_type.T.plot(kind='bar', ax=axes[1], color=['darkred', 'gold'], alpha=0.8)
axes[1].set_title('Quality Distribution by Wine Type', fontsize=14, fontweight='bold')
axes[1].set_xlabel('Quality Score')
axes[1].set_ylabel('Count')
axes[1].legend(['Red Wine', 'White Wine'])
axes[1].grid(True, alpha=0.3)

# Quality categories for Random Forest
def categorize_quality(quality):
    if quality <= 5:
        return 'Low (3-5)'
    elif quality <= 7:
        return 'Medium (6-7)'
    else:
        return 'High (8-9)'

wine_data['quality_category'] = wine_data['quality'].apply(categorize_quality)
category_counts = wine_data['quality_category'].value_counts()
category_counts.plot(kind='pie', ax=axes[2], autopct='%1.1f%%', colors=['#ff9999', '#66b3ff', '#99ff99'])
axes[2].set_title('Quality Categories for Classification', fontsize=14, fontweight='bold')
axes[2].set_ylabel('')

plt.tight_layout()
plt.show()
```

@fig-quality-distribution shows the distribution of wine quality scores. Most wines receive medium quality scores (5-7), with very few at the extremes.

```{python}
#| echo: false
#| label: fig-feature-correlations
#| fig-cap: "Feature correlations with wine quality score. Green bars indicate positive correlations, red bars indicate negative correlations"
#| fig-width: 10
#| fig-height: 8

# Feature correlations with quality
feature_cols = wine_data.columns.drop(['quality', 'quality_category'])
correlations = wine_data[feature_cols].corrwith(wine_data['quality']).sort_values(ascending=False)

plt.figure(figsize=(10, 8))
colors = ['green' if x > 0 else 'red' for x in correlations.values]
correlations.plot(kind='barh', color=colors)
plt.title('Feature Correlations with Wine Quality', fontsize=14, fontweight='bold')
plt.xlabel('Correlation Coefficient')
plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

@fig-feature-correlations reveals that alcohol content shows the strongest positive correlation with quality, while volatile acidity shows the strongest negative correlation.

```{python}
#| echo: false
#| label: fig-correlation-heatmap
#| fig-cap: "Correlation heatmap showing relationships between all physicochemical features"
#| fig-width: 12
#| fig-height: 10

# Feature correlation heatmap
plt.figure(figsize=(12, 10))
correlation_matrix = wine_data[feature_cols].corr()
mask = np.triu(np.ones_like(correlation_matrix), k=1)
sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', 
            cmap='coolwarm', center=0, square=True, linewidths=1)
plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()
```

@fig-correlation-heatmap displays the correlations between all physicochemical features, helping identify potential multicollinearity issues for our models.


### 3. Data Preprocessing for Random Forest

The wine quality dataset was prepared for analysis by separating the features (physicochemical properties of the wine) from the target variable (the quality category). The quality categories were encoded numerically so that the machine learning model could process them.

To ensure the model learned effectively, the dataset was split into a training set (used to train the model) and a test set (used to evaluate the model's performance on unseen data). The split was stratified, meaning the proportions of each quality category in the training and test sets match the overall dataset distribution. This helps the model fairly learn from all classes of wine quality.

```{python}
import pandas as pd
import numpy as np
import pickle

#access saved data split
with open("../results/splits.pkl", "rb") as f:
    X_train, X_test, y_train, y_test, le = pickle.load(f)

# Data split summary
train_samples = len(X_train)
test_samples = len(X_test)
n_features = X_train.shape[1]

# Class distributions

classes = le.classes_
train_counts = np.unique(y_train, return_counts=True)[1]
test_counts = np.unique(y_test, return_counts=True)[1]

train_percentages = (train_counts / train_samples * 100).round(1)
test_percentages = (test_counts / test_samples * 100).round(1)

# Convert NumPy types -> Python types
train_counts = [int(x) for x in train_counts]
test_counts = [int(x) for x in test_counts]

train_percentages = [float(x) for x in train_percentages]
test_percentages = [float(x) for x in test_percentages]

```


```{python}
#| label: tbl-summary_df
#| tbl-cap: Summary of Data Split

import pandas as pd

# Summary table
summary_df = pd.DataFrame({
    "Dataset": ["Training Set", "Test Set"],
    "Samples": [train_samples, test_samples],
    "Features": [n_features, n_features]
})

summary_df
```

```{python}
#| label: tbl-train_dist_df
#| tbl-cap: Training set class distribution table

import pandas as pd

train_dist_df = pd.DataFrame({
"Quality Category": classes,
"Samples": train_counts,
"Percentage": train_percentages
})

train_dist_df
```


```{python}
#| label: tbl-test_dist_df
#| tbl-cap: Test set class distribution table

import pandas as pd

test_dist_df = pd.DataFrame({
"Quality Category": classes,
"Samples": test_counts,
"Percentage": test_percentages
})

test_dist_df
```


This preprocessing ensured that the model would learn from all classes and could generalize well to new, unseen wines.


### 4. Random Forest Model Development

A Random Forest classifier, consisting of 100 decision trees, was trained on the processed data to predict wine quality. The model’s performance was evaluated using several metrics:

- Training Accuracy: Measures how well the model fits the training data.

- Test Accuracy: Measures how well the model generalizes to new, unseen data.

- Out-of-Bag (OOB) Score: An internal validation method for Random Forests, giving an unbiased estimate of model performance.

- Cross-Validation Accuracy: Performance measured by repeatedly splitting the training set into subsets to validate the model, ensuring stability of results.

```{python}
# rain Random Forest and store metrics

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score

# Build and train Random Forest

# rf_model = RandomForestClassifier(n_estimators=100, random_state=42, oob_score=True, n_jobs=1)
# rf_model.fit(X_train, y_train)

with open("../results/models/rf_wine_models.pkl", "rb") as f:
    rf_model = pickle.load(f)

# Predictions on train and test sets

y_pred_train = rf_model.predict(X_train)
y_pred_test = rf_model.predict(X_test)

# Performance metrics

train_accuracy = accuracy_score(y_train, y_pred_train)
test_accuracy = accuracy_score(y_test, y_pred_test)
oob_score = rf_model.oob_score_

# cross-validation on training set
cv_scores = cross_val_score(rf_model, X_train, y_train, cv=4, scoring='accuracy')
cv_mean = float(cv_scores.mean())
cv_std = float(cv_scores.std() * 2 ) # 95% CI approximation

```


```{python}
#| label: tbl-rf_perf_df
#| tbl-cap: Random Forest performance table

import pandas as pd

rf_perf_df = pd.DataFrame({
"Metric": [
"Training Accuracy",
"Test Accuracy",
"Out-of-Bag (OOB) Score",
"Cross-Validation Accuracy"
],
"Value": [
round(train_accuracy, 4),
round(test_accuracy, 4),
round(oob_score, 4),
f"{round(cv_mean, 4)} ± {round(cv_std, 4)}"
]
})

rf_perf_df

```


Note on class-specific metrics: Some classes, particularly the very high-quality wines, have very few samples. During evaluation, metrics such as precision and recall may be undefined for these classes because the model predicted zero samples in them. This does not affect the overall accuracy or the main performance conclusions of the model.

```{python}
#Remove this if can't fix threshold
# Identify rare classes in the training or test set

threshold = 0.5  # e.g., classes with less than 5% of samples
rare_classes = [cls for cls, pct in zip(classes, train_percentages) if pct < threshold]

if rare_classes:
    print(
    "Note on class-specific metrics: Some classes with very few samples "
    f"({', '.join(rare_classes)}) may have undefined precision or recall, "
    "because the model predicted zero instances in those classes. "
    "This does not affect overall accuracy or main performance conclusions."
    )

    #print("All classes have sufficient samples; no class-specific warnings needed.")

```

These results indicate that the model fits the training data very well while maintaining good predictive performance on unseen data. This demonstrates that the Random Forest classifier is effective in predicting wine quality based on physicochemical properties.


```{python}

```

```{python}

```


### 5. Model Evaluation 

#### 5.1. Baseline Random Forest: Confusion Matrix and Classification Report

In this step, we evaluate the performance of the trained Random Forest classifier using a confusion matrix and a detailed classification report. The evaluation script loads the cleaned dataset, applies the previously trained model, and computes key metrics such as precision, recall, and F1-score for each wine-quality class.


##### 5.1.1 Confusion Matrix

The confusion matrix provides a visual summary of how well the model distinguishes between the different categories, while the classification report breaks down performance per class.

Due to the natural class imbalance in the wine-quality dataset, some classes have very few samples, which results in undefined precision and recall values (these appear as warnings during evaluation). Despite this, the evaluation helps identify where the model performs well and where predictions remain challenging.

The model was evaluated using the test set generated during preprocessing.

```{python}
#| label: fig-confusion-matrix
#| fig-cap: "Confusion Matrix for Random Forest Classifier"
#| fig-align: center

from PIL import Image
import matplotlib.pyplot as plt

img_path = "../results/evaluation/confusion_matrix_random_forest.png"

img = Image.open(img_path)
plt.imshow(img)
plt.axis("off")
plt.title("Confusion Matrix")
plt.show()

```


###### Interpretation of Confusion Matrix

The model correctly classifies most Medium (6–7) and Low (3–5) quality wines, which form the majority of the dataset. However, it rarely predicts the High (8–9) class correctly, reflecting the very small number of training examples in this category.


##### 5.1.2 Classification Report

The following classification report was generated by the evaluation script.

```{python}
#| label: classification-report
#| tbl-cap: "Classification Report from Random Forest Model"

import os

report_path = "../results/evaluation/classification_report.txt"

if os.path.exists(report_path):
    with open(report_path, "r") as f:
        report_text = f.read()

    print(report_text)
else:
    print(f"Classification report not found at: {report_path}")
```


###### Interpretation of Classification Report

The overall accuracy of 75% suggests that the model performs reasonably well.
The weighted averages are high because most samples belong to the Medium and Low classes, which the model predicts effectively.
However, the macro average is much lower (≈0.49), indicating that the model performs poorly on the rare High (8–9) class.


##### 5.1.3 Discussion

Overall, the Random Forest model is suitable for predicting Low and Medium wine quality but not reliable for detecting High-quality wines. Improving performance on the minority class may require techniques such as resampling, class weighting, or training a different model.


#### 5.2 Feature Importance Analysis

To better understand how the Random Forest classifier makes its predictions, we analyzed the importance of each input feature. Tree-based models such as Random Forests compute importance scores based on how much each feature reduces impurity across all decision trees in the ensemble.

The analysis script loads the processed dataset and the trained model, extracts the feature importances, and generates both a visualization and a ranked table of feature contributions. This allows us to identify which chemical properties of the wine were most influential in determining wine-quality categories.

```{python}
#| label: fig-feature-importance
#| fig-cap: "Feature Importance Scores from the Random Forest Model"
#| fig-align: center

from PIL import Image
import matplotlib.pyplot as plt

img_path = "../results/evaluation/feature_importance_random_forest.png"

img = Image.open(img_path)
plt.imshow(img)
plt.axis("off")
plt.title("Feature Importance (Random Forest)", fontsize=14)
plt.show()

```

The top features identified by the model are shown below:

```{python}
#| label: tbl-feature-importance
#| tbl-cap: "Top 5 Most Important Features Identified by the Random Forest Model"

import pandas as pd

table_path = "../results/evaluation/feature_importance_table.csv"
fi_table = pd.read_csv(table_path)

fi_table.head(5)

```

```{python}
# Dynamically extract the top 2 features
top2 = fi_table.head(2)
top2_features = " and ".join(top2['feature'].tolist())
```

The feature importance analysis above indicates that `{python} top2_features` features play a substantial role in distinguishing wine-quality categories in this dataset. The remaining features contribute smaller but still meaningful incremental value.


#### 5.3 Hyperparameter Tuning of Random Forest

To improve predictive performance, the Random Forest classifier was tuned to optimize its hyperparameters for the task of predicting wine quality categories. The tuning process aimed to maximize model accuracy while ensuring balanced performance across all quality classes, particularly addressing the underrepresented "High (8-9)" category.

Stratified cross-validation was used to evaluate different combinations of hyperparameters, and class imbalance was handled using balanced class weights to ensure the minority class contributed proportionally to the training process. The resulting optimized model demonstrates improved generalization and robust performance on unseen test data.


##### 5.3.1 Class Distribution

The distribution of wine quality categories in the training and test sets is shown in Tables @tbl-train_dist_df and @tbl-test_dist_df (created during the data preprocessing stage). These tables summarize both the number of samples and the percentage of each quality class in the respective datasets.

As shown, the dataset exhibits a noticeable class imbalance, particularly for the minority "High (8-9)" category. To address this, the Random Forest classifier was trained using class_weight="balanced" to ensure the model adequately learns from underrepresented classes without being biased toward the majority categories.


#### 5.3.2 Hyperparameter Tuning Results

The GridSearchCV process explored 324 combinations of hyperparameters over 5-fold stratified cross-validation. The best hyperparameters identified are shown in Table @tbl-best_params_df.

```{python}
#| label: tuning-results
#| echo: false
import json
from pathlib import Path

# Paths

metrics_path = Path("../results/evaluation/rf_test_metrics.json")
tuning_summary_path = Path("../results/evaluation/rf_hyperparameter_tuning_results.txt")

# Load test metrics

with open(metrics_path, "r") as f:
    test_metrics = json.load(f)

# Read tuning summary to extract best params and CV score

best_params = {}
cv_score = None
with open(tuning_summary_path, "r") as f:
    for line in f:
        if line.startswith("  "):
            param, value = line.strip().split(": ")
            best_params[param] = value
        elif "Best cross-validation accuracy" in line:
            cv_score = float(line.strip().split(": ")[1])
```


```{python}
#| label: tbl-best_params_df
#| tbl-cap: Best Random Forest Hyperparameters
import pandas as pd

best_params_df = pd.DataFrame({
"Hyperparameter": list(best_params.keys()),
"Value": list(best_params.values())
})
best_params_df

```

```{python}
#| label: cv-score
cv_score

```


The best cross-validation accuracy achieved was `{python} cv_score`, indicating consistent performance across the training folds.

These hyperparameters balance tree complexity and generalization. For example, max_depth of `{python} best_params["max_depth"]` allows sufficiently deep trees, while `{python} best_params["min_samples_leaf"]` prevents overfitting small branches. Limiting the number of features considered at each split using `{python} best_params["max_features"]` helps improve model robustness.


#### 5.3.3 Test Set Performance

The performance of the optimized Random Forest model was evaluated on the held-out test set. The key metrics are summarized in Table @tbl-test_metrics_df.

```{python}
#| label: tbl-test_metrics_df
#| tbl-cap: Optimized Random Forest Test Set Performance

import pandas as pd

test_metrics_df = pd.DataFrame({
    "Metric": ["Accuracy", "Precision", "Recall", "F1-Score"],
    "Value": [
        round(test_metrics["accuracy"],3),
        round(test_metrics["precision"], 3),
        round(test_metrics["recall"],3),
        round(test_metrics["f1_score"],3)
    ]
})

test_metrics_df
```

The accuracy of `{python} round(test_metrics["accuracy"],3)` indicates that the model generalizes well to unseen data. The balanced precision (`{python} round(test_metrics["precision"], 3)`) and recall (`{python} round(test_metrics["recall"],3)`) suggest that the model does not disproportionately favor any class despite the underlying class imbalance. The slightly lower F1-score (`{python} round(test_metrics["f1_score"],3)`) reflects some trade-offs in correctly classifying the minority "High (8-9)" wines, which is expected given their rarity.


#### 5.3.4 Confusion Matrix

The confusion matrix @fig-conf-matrix summarizes the model’s predictions on the test set across all wine quality categories. It provides detailed insight into per-class performance, highlighting where the model succeeds and where misclassifications occur.

The confusion matrix (Figure 1) illustrates how the model performs across the three wine quality categories. The use of balanced class weights helped the classifier better identify the minority "High (8-9)" wines while maintaining accuracy on the more common categories.

![](../results/evaluation/confusion_matrix_random_forest_optimized.png){#fig-conf-matrix width=60%}

Key observations from the confusion matrix include:

- The model correctly classifies the majority of "Medium (6-7)" and "Low (3-5)" wines, which are the dominant classes in the dataset.

- Importantly, the previously underrepresented "High (8-9)" wines, which had virtually no correct predictions before hyperparameter tuning of the Random Forest model, are now having some values classified more accurately. This improvement is largely due to the use of balanced class weights during training and hyperparameter tuning.

- Some misclassifications still occur for the "High (8-9)" category, reflecting its rarity and the inherent difficulty in predicting minority classes. However, the optimized Random Forest achieves a meaningful trade-off between overall accuracy and minority class recall.

Key Takeaways: 

- Hyperparameter tuning with GridSearchCV, combined with class balancing, has improved model generalization and robustness.

- The optimized Random Forest can now handle imbalanced class distributions effectively, providing both high accuracy and better sensitivity for the minority class.


###### Why Random Forest Excelled

1. **Ensemble Advantage**: Aggregating predictions from multiple decision trees reduces overfitting and improves generalization compared to single models.
2. **Feature Interactions**: Captures complex relationships between chemical properties that influence wine quality.
3. **Robustness**: Handles class imbalance and potential outliers in chemical measurements without extensive preprocessing.

```{python}
# Dynamically extract the top 2 features
# top2 = fi_table.head(2)
# top2_features = " and ".join(top2['feature'].tolist())
alcohol_importance = round(float(fi_table.head(1)['importance'].iloc[0]), 4)

```

### Key Findings

- **Alcohol content** emerged as the most important predictor (importance: `{python} alcohol_importance`), consistent with wine industry knowledge.
- **Volatile acidity** and **density** were also important predictors.
- The optimized Random Forest model achieved a balanced performance across classes with test accuracy of `{python} cv_score` and well-balanced precision and recall.


### Practical Applications

1. **Quality Control**: Winemakers can use the model to predict quality during production
2. **Process Optimization**: Focus on controlling key chemical properties identified by feature importance
3. **Objective Assessment**: Complement subjective expert ratings with data-driven predictions


### Limitations and Future Work

1. **Dataset Scope**: Limited to Portuguese "Vinho Verde" wines.
2. **Feature Set**: Additional sensory or temporal data could further improve predictions.
3. **Class Imbalance**: High-quality wines (8-9) represent only ~3% of samples; model tuning with balanced class weights helped reduce bias toward majority classes.
4. **Future Directions**: 
   - Extend the model to other wine regions and varieties.
   - Incorporate temporal data and wine aging effects.
   - Explore additional techniques for handling class imbalance (e.g., SMOTE).
   - Develop real-time quality monitoring systems.


### Conclusion

This project demonstrated that the Random Forest classifier is effective for predicting wine quality from physicochemical properties. The optimized model achieved a test accuracy of `{python} cv_score` with well-balanced precision and recall, showing good generalization to unseen data. Feature importance analysis highlighted alcohol content, volatile acidity, and density as key predictors, providing actionable insights for wine production optimization. Random Forest's advantages—handling non-linear relationships, managing class imbalance, and providing interpretable features—make it a suitable choice for wine quality prediction in practice.


## References

::: {#refs}
:::




