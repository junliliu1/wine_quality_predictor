---
title: Predicting Wine Quality using Random Forest Classifier
author: "Junli Liu, Luis Alvarez, Purity Jangaya & Jimmy Wang"
date: "2025/11/20"
jupyter: python3

format:
  html:
    toc: true
    toc-depth: 4
    #output-file: index.html
  pdf:
    toc: true
    toc-depth: 4

bibliography: references.bib

execute:
  echo: false
  warning: false

editor: source
---

## Summary

This project implements a Random Forest classifier to predict wine quality based on physicochemical properties. Using the Wine Quality dataset from the UCI Machine Learning Repository (6,497 samples with 11 features, reduced to 5,320 after removing duplicates), we develop a robust prediction model that leverages the ensemble learning capabilities of Random Forest. Our analysis demonstrates that Random Forest effectively handles the non-linear relationships between chemical properties and wine quality, achieving an accuracy of approximately 74.6% on the test set. The model identifies alcohol content, volatile acidity, and density as the most influential factors in determining wine quality. This work provides valuable insights for wine producers to optimize production processes and maintain consistent quality standards.

## Introduction

Wine quality assessment traditionally relies on subjective evaluation by human experts. This project explores the potential of machine learning, specifically Random Forest classification, to predict wine quality from objective physicochemical measurements.

### Why Random Forest?

We selected Random Forest as our primary algorithm for several reasons:

1. **Ensemble Learning**: Combines multiple decision trees to reduce overfitting and improve generalization
2. **Feature Importance**: Provides built-in feature importance metrics for understanding wine quality factors
3. **Robustness**: Handles outliers and noise effectively without extensive preprocessing
4. **Non-linear Relationships**: Captures complex interactions between chemical properties
5. **No Scaling Required**: Works well with features at different scales
6. **Out-of-Bag (OOB) Error**: Provides unbiased error estimates without separate validation set

### Research Questions

1. Can Random Forest effectively predict wine quality from physicochemical properties?
2. Which chemical properties are most important for determining wine quality?
3. How does Random Forest performance compare to other classification methods?
4. What are the optimal hyperparameters for our Random Forest model?

## Methods

### Data

**Dataset**: Wine Quality Dataset (Cortez et al., 2009)
- **Red wine**: 1,599 samples
- **White wine**: 4,898 samples
- **Total**: 6,497 samples

**Features (11 physicochemical properties)**:
1. Fixed acidity (g/dm³)
2. Volatile acidity (g/dm³)
3. Citric acid (g/dm³)
4. Residual sugar (g/dm³)
5. Chlorides (g/dm³)
6. Free sulfur dioxide (mg/dm³)
7. Total sulfur dioxide (mg/dm³)
8. Density (g/cm³)
9. pH
10. Sulphates (g/dm³)
11. Alcohol (% vol.)

**Target**: Quality score (3-9, originally 0-10 scale)

**Citation**: P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. Decision Support Systems, 47(4):547-553, 2009.

### Analysis Pipeline

1. **Data Preparation**
   - Load and combine red/white wine datasets
   - Handle class imbalance through stratified splitting
   - Create quality categories (Low: 3-5, Medium: 6-7, High: 8-9)

2. **Exploratory Data Analysis**
   - Analyze quality distribution
   - Examine feature correlations
   - Identify potential predictors

3. **Random Forest Implementation**
   - Build initial Random Forest with 100 trees
   - Analyze out-of-bag (OOB) error
   - Extract feature importances
   - Evaluate model performance

4. **Hyperparameter Optimization**
   - Grid search for optimal parameters:
     - n_estimators (number of trees)
     - max_depth (tree depth)
     - min_samples_split
     - min_samples_leaf
     - max_features

5. **Model Comparison**
   - Benchmark against Logistic Regression, SVM, and Gradient Boosting
   - Validate Random Forest superiority

6. **Final Evaluation**
   - Test set performance
   - Confusion matrix analysis
   - Feature importance interpretation

## Implementation

```{python}
# Import required libraries
# import pandas as pd
# import numpy as np
# import matplotlib.pyplot as plt
# import seaborn as sns
# from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold
# from sklearn.preprocessing import StandardScaler, LabelEncoder
# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
# from sklearn.linear_model import LogisticRegression
# from sklearn.svm import SVC
# from sklearn.metrics import (
#     classification_report, confusion_matrix, accuracy_score,
#     precision_recall_fscore_support, roc_auc_score, roc_curve
# )
# import pandera as pa
# from pandera import Column, Check, DataFrameSchema
# import warnings
# warnings.filterwarnings('ignore')

# # Set random seed for reproducibility
# np.random.seed(42)

# # Set plotting style
# sns.set_theme(style="whitegrid")
# sns.set_palette("husl")

# print("Libraries loaded successfully!")
```

### 1. Data Loading and Preparation

To begin our analysis, we downloaded the two wine quality datasets provided by Cortez et al. (2009). These files contain measurements for red and white wine samples, including physicochemical properties and expert-assigned quality scores.

The data were downloaded using our automated script, which saves the raw files into the data/raw directory.
The output from this script is shown in Table @tbl-download-log.

The two datasets downloaded were:

- winequality-red.csv
- winequality-white.csv

These files form the foundation of the analysis that follows.

```{python}
# # Load datasets
# red_wine = pd.read_csv('data/raw/winequality-red.csv', sep=';')
# white_wine = pd.read_csv('data/raw/winequality-white.csv', sep=';')
# data/raw/winequality-red.csv

# # Add wine type indicator
# red_wine['wine_type'] = 0  # 0 for red
# white_wine['wine_type'] = 1  # 1 for white

# # Combine datasets
# wine_data = pd.concat([red_wine, white_wine], ignore_index=True)

# print("Dataset Overview:")
# print(f"Total samples: {len(wine_data)}")
# print(f"Red wine: {len(red_wine)} samples ({len(red_wine)/len(wine_data)*100:.1f}%)")
# print(f"White wine: {len(white_wine)} samples ({len(white_wine)/len(wine_data)*100:.1f}%)")
# print(f"Features: {wine_data.shape[1] - 1}")
# print(f"\nQuality distribution:")
# print(wine_data['quality'].value_counts().sort_index())
```

```{python}
# # Check for missing values and data types
# print("Data Quality Check:")
# print(f"Missing values: {wine_data.isnull().sum().sum()}")
# print(f"\nData types:")
# print(wine_data.dtypes)
# print(f"\nBasic statistics:")
# wine_data.describe()
```

### 1.1 Data Validation

Before proceeding with analysis, we perform comprehensive data validation checks following the [Data Validation Checklist](https://ubc-dsci.github.io/reproducible-and-trustworthy-workflows-for-data-science/lectures/130-data-validation.html#data-validation-checklist) to ensure data quality and integrity.

```{python}
# # Data Validation using Pandera
# import pandera as pa
# from pandera import Column, Check, DataFrameSchema

# # Define expected column names for raw wine data
# EXPECTED_COLUMNS = [
#     'fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',
#     'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',
#     'pH', 'sulphates', 'alcohol', 'quality'
# ]

# # Define valid quality score range
# VALID_QUALITY_RANGE = (3, 9)

# # Define reasonable ranges for physicochemical properties based on domain knowledge
# FEATURE_RANGES = {
#     'fixed acidity': (0, 20),        # g/dm³ - typical wine range
#     'volatile acidity': (0, 2),       # g/dm³ - acetic acid
#     'citric acid': (0, 2),            # g/dm³
#     'residual sugar': (0, 100),       # g/dm³
#     'chlorides': (0, 1),              # g/dm³
#     'free sulfur dioxide': (0, 300),  # mg/dm³
#     'total sulfur dioxide': (0, 500), # mg/dm³
#     'density': (0.9, 1.1),            # g/cm³ - close to water
#     'pH': (2, 5),                     # typical wine pH range
#     'sulphates': (0, 3),              # g/dm³
#     'alcohol': (5, 20)                # % vol.
# }

# print("Data Validation Configuration:")
# print(f"Expected columns: {len(EXPECTED_COLUMNS)}")
# print(f"Valid quality range: {VALID_QUALITY_RANGE}")
# print("Feature ranges defined for anomaly detection.")
```

```{python}
#| scrolled: true
# Validation Check 1 & 2: File Format and Column Names
# print("="*60)
# print("VALIDATION CHECK 1 & 2: File Format and Column Names")
# print("="*60)

# def validate_column_names(df, expected_cols, dataset_name):
#     """Validate that all expected columns are present."""
#     actual_cols = set(df.columns)
#     expected_set = set(expected_cols)
    
#     missing = expected_set - actual_cols
#     extra = actual_cols - expected_set
    
#     if missing:
#         raise ValueError(f"{dataset_name}: Missing columns: {missing}")
#     if extra:
#         print(f"  WARNING: {dataset_name} has extra columns: {extra}")
    
#     print(f"  {dataset_name}: All {len(expected_cols)} expected columns present")
#     return True

# # Validate red wine
# validate_column_names(red_wine.drop('wine_type', axis=1), EXPECTED_COLUMNS, "Red wine")

# # Validate white wine
# validate_column_names(white_wine.drop('wine_type', axis=1), EXPECTED_COLUMNS, "White wine")

# # Validate combined dataset
# validate_column_names(wine_data.drop(['wine_type', 'quality_category'], axis=1, errors='ignore'), 
#                      EXPECTED_COLUMNS, "Combined dataset")

# print("\n[PASSED] File format and all column names validation")
```

```{python}
# Validation Check 3: Empty Observations
# print("="*60)
# print("VALIDATION CHECK 3: Empty Observations")
# print("="*60)

# def check_empty_observations(df, dataset_name):
#     """Check for completely empty rows."""
#     empty_rows = df.isnull().all(axis=1).sum()
#     if empty_rows > 0:
#         raise ValueError(f"{dataset_name}: Found {empty_rows} completely empty rows")
#     print(f"  {dataset_name}: No completely empty observations found")
#     return True

# check_empty_observations(red_wine, "Red wine")
# check_empty_observations(white_wine, "White wine")
# check_empty_observations(wine_data, "Combined dataset")

# print("\n[PASSED] No empty observations found")
```

```{python}
# Validation Check 4: Missingness
# print("="*60)
# print("VALIDATION CHECK 4: Missingness")
# print("="*60)

# MISSING_THRESHOLD = 0.05  # Allow up to 5% missing values per column

# def check_missingness(df, threshold, dataset_name):
#     """Check that missing values do not exceed threshold."""
#     missing_pct = df.isnull().mean()
#     violations = missing_pct[missing_pct > threshold]
    
#     if len(violations) > 0:
#         raise ValueError(f"{dataset_name}: Columns exceed {threshold*100}% missing threshold:\n{violations}")
    
#     total_missing = df.isnull().sum().sum()
#     print(f"  {dataset_name}: Total missing values = {total_missing}")
#     print(f"  {dataset_name}: Max missing % per column = {missing_pct.max()*100:.2f}%")
#     return True

# check_missingness(red_wine, MISSING_THRESHOLD, "Red wine")
# check_missingness(white_wine, MISSING_THRESHOLD, "White wine")
# check_missingness(wine_data, MISSING_THRESHOLD, "Combined dataset")

# print(f"\n[PASSED] All columns below {MISSING_THRESHOLD*100}% missing threshold")
```

```{python}
# Validation Check 5: Data Types
# print("="*60)
# print("VALIDATION CHECK 5: Data Types")
# print("="*60)

# EXPECTED_DTYPES = {
#     'fixed acidity': 'float64',
#     'volatile acidity': 'float64',
#     'citric acid': 'float64',
#     'residual sugar': 'float64',
#     'chlorides': 'float64',
#     'free sulfur dioxide': 'float64',
#     'total sulfur dioxide': 'float64',
#     'density': 'float64',
#     'pH': 'float64',
#     'sulphates': 'float64',
#     'alcohol': 'float64',
#     'quality': 'int64'
# }

# def check_data_types(df, expected_dtypes, dataset_name):
#     """Validate that columns have expected data types."""
#     type_errors = []
#     for col, expected_type in expected_dtypes.items():
#         if col in df.columns:
#             actual_type = str(df[col].dtype)
#             if actual_type != expected_type:
#                 type_errors.append(f"  {col}: expected {expected_type}, got {actual_type}")
    
#     if type_errors:
#         raise ValueError(f"{dataset_name}: Data type mismatches:\n" + "\n".join(type_errors))
    
#     print(f"  {dataset_name}: All columns have correct data types")
#     return True

# check_data_types(wine_data, EXPECTED_DTYPES, "Combined dataset")

# print("\n[PASSED] Data types validation")
```

```{python}
# Validation Check 6: Duplicate Observations
# print("="*60)
# print("VALIDATION CHECK 6: Duplicate Observations")
# print("="*60)

# def check_duplicates(df, dataset_name):
#     """Check for and report duplicate rows."""
#     duplicates = df.duplicated().sum()
#     duplicate_pct = duplicates / len(df) * 100
    
#     print(f"  {dataset_name}: {duplicates} duplicate rows ({duplicate_pct:.2f}%)")
    
#     return duplicates

# dup_red = check_duplicates(red_wine, "Red wine")
# dup_white = check_duplicates(white_wine, "White wine")
# dup_combined = check_duplicates(wine_data, "Combined dataset")

# # Remove duplicates from combined dataset
# print(f"\nRemoving {dup_combined} duplicate rows from combined dataset...")
# wine_data = wine_data.drop_duplicates().reset_index(drop=True)
# print(f"Dataset size after removing duplicates: {len(wine_data)} samples")

# # Verify no duplicates remain
# dup_after = wine_data.duplicated().sum()
# print(f"Duplicates remaining: {dup_after}")

# print("\n[PASSED] Duplicate observations removed")
```

```{python}
# Validation Check 7: No Outliers neither Anamalous Values
# print("="*60)
# print("VALIDATION CHECK 7: NO OUTLIERS OR ANAMALOUS VALUES")
# print("="*60)

# def check_value_ranges(df, feature_ranges, dataset_name):
#     """Check for values out of domain ranges."""
#     outliers={}

#     for column, (min_val, max_val) in feature_ranges.items():
#         below_min = (df[column] < min_val).sum()
#         above_max = (df[column] > max_val).sum()

#         if below_min > 0 or above_max > 0:
#                 outliers[column] = {
#                     'below_min': below_min,
#                     'above_max': above_max,
#                     'expected_range': (min_val, max_val),
#                     'actual_range': (df[column].min(), df[column].max())
#                 }
        
#     if outliers:
#         print(f"  {dataset_name}: Found values outside expected ranges:")
#         for column, info in outliers.items():
#             print(f"    {column}: expected [{info['expected_range'][0]}, {info['expected_range'][1]}], "
#                   f"got [{info['actual_range'][0]:.3f}, {info['actual_range'][1]:.3f}]")
#             print(f"      - {info['below_min']} values below min, {info['above_max']} values above max")
#     else:
#         print(f"  {dataset_name}: All values within expected ranges")
    
#     return outliers

# outliers = check_value_ranges(wine_data, FEATURE_RANGES, "Combined dataset")

# if not outliers:
#     print("\n[PASSED] No anomalous values neither outliers detected")
# else:
#     print("\n[WARNING] Some values outside expected ranges - investigate before proceeding")
```

```{python}
# Validation Check 8: Correct Category Levels (for categorical features)
# print("="*60)
# print("VALIDATION CHECK 8: CATEGORY LEVELS")
# print("="*60)

# def check_category_levels(df, column, expected_values, dataset_name):
#     """Validate categorical column has expected values"""
#     actual_values = set(df[column].unique())
#     expected_set = set(expected_values)

#     unexpected_set = actual_values - expected_set
#     missing_set = expected_set - actual_values

#     if unexpected_set:
#         print(f"\n WARNING: {dataset_name} {column} has unexpected values: {unexpected_set}")
#     if missing_set:
#         print(f"\n  INFO: {dataset_name} {column} missing expected values: {missing_set}")

#     print(f"\n {dataset_name} {column} has {len(actual_values)} unique values: {sorted(actual_values)}")
#     return len(unexpected_set) == 0

# # Check quality values (should be integers 3-9)
# expected_quality = list(range(3, 10))  # 3, 4, 5, 6, 7, 8, 9
# check_category_levels(wine_data, 'quality', expected_quality, "Combined dataset")

# # Check wine_type values (should be 0 or 1)
# expected_wine_type = [0, 1]
# check_category_levels(wine_data, 'wine_type', expected_wine_type, "Combined dataset")

# print("\n[PASSED] Category levels validation")     
```

```{python}
# Validation Check 9: Target/response variable follows expected distribution
# print("="*60)
# print("VALIDATION CHECK 9: Target Variable follows expected distribution")
# print("="*60)

# def check_target_distribution(df, target_col, dataset_name):
#     """Analyze target variable distribution for imbalance."""
#     distribution = df[target_col].value_counts().sort_index()
#     proportions = df[target_col].value_counts(normalize=True).sort_index()
    
#     print(f"\n  {dataset_name} - {target_col} distribution:")
#     for val, count in distribution.items():
#         print(f"    {val}: {count} ({proportions[val]*100:.1f}%)")
    
#     # Check for severe imbalance (any class < 1%)
#     min_proportion = proportions.min()
#     max_proportion = proportions.max()
#     imbalance_ratio = max_proportion / min_proportion
    
#     print(f"\n  Class imbalance ratio: {imbalance_ratio:.1f}:1")
    
#     if min_proportion < 0.01:
#         print(f"  WARNING: Severe class imbalance detected (min class = {min_proportion*100:.2f}%)")
#     elif min_proportion < 0.05:
#         print(f"  WARNING: Moderate class imbalance detected (min class = {min_proportion*100:.2f}%)")
#     else:
#         print(f"  Class distribution is acceptable for modeling")
    
#     return imbalance_ratio

# imbalance = check_target_distribution(wine_data, 'quality', "Combined dataset")
```

```{python}
# Validation Check 10 & 11: Feature Correlations (Target-Feature and Feature-Feature)
# print("="*60)
# print("VALIDATION CHECK 10 & 11: Feature Correlations")
# print("="*60)

# def check_correlations(df, target_col, feature_cols, dataset_name):
#     """Check for suspicious correlations that might indicate data issues"""

#     #Target-Feature correlations
#     print(f"\n {dataset_name} - Target-Feature Correlations:")
#     target_corr = df[feature_cols].corrwith(df[target_col]).abs().sort_values(ascending=False)

#     #Flag if any feature has very high correlation with target(potential leakage)
#     high_target_corr = target_corr[target_corr > 0.9]
#     if len(high_target_corr) > 0:
#         print(f"    WARNING: Features with suspiciously high target correlation (>0.9):")
#         for feat, corr in high_target_corr.items():
#             print(f"    {feat}: {corr:.3f}")
#     else:
#         print(f"    No suspiciously high target correlations detected")
    
#     print(f"    Highest target correlation: {target_corr.index[0]} ({target_corr.iloc[0]:.3f})")
#     print(f"  Lowest target correlation: {target_corr.index[-1]} ({target_corr.iloc[-1]:.3f})")

#     #Feature-Feature correlations
#     print(f"\n {dataset_name} - Feature-Feature Correlations:")
#     corr_matrix = df[feature_cols].corr().abs()

#     #Find highly correlated feature pairs (excluding diagonal)
#     high_corr_pairs = []
#     for i in range(len(feature_cols)):
#         for j in range(i+1, len(feature_cols)):
#             corr = corr_matrix.iloc[i, j]
#             if corr > 0.8:
#                 high_corr_pairs.append((feature_cols[i], feature_cols[j], corr))
    
#     if high_corr_pairs:
#         print(f"    Highly correlated feature pairs (>0.8):")
#         for f1, f2, corr in sorted(high_corr_pairs, key=lambda x: -x[2]):
#             print(f"    {f1} <-> {f2}: {corr:.3f}")
    
#     else:
#         print(f"   No highly correlated features pairs (>0.8) detected")
    
#     return target_corr, high_corr_pairs

# feature_cols = [col for col in EXPECTED_COLUMNS if col != 'quality']
# target_corr, high_corr = check_correlations(wine_data, 'quality', feature_cols, "Combined dataset")

# print("\n[INFO] Correlation analysis complete - no data leakage detected")

```

```{python}
# Comprehensive Pandera Schema Validation
# print("="*60)
# print("COMPREHENSIVE SCHEMA VALIDATION (Pandera)")

# Define schema for wine data validation
# Note: Duplicate check is excluded here because we already removed duplicates 
# from the full dataset (including wine_type) in cell 17. When validating only 
# the original columns (without wine_type), some rows may appear as duplicates 
# since different wine types can have identical physiocochemical properties

# wine_schema = pa.DataFrameSchema({
#     "fixed acidity": Column(float, Check.in_range(0, 20), nullable=False), 
#     "volatile acidity": Column(float, Check.in_range(0, 2), nullable=False),
#     "citric acid": Column(float, Check.in_range(0, 2), nullable=False),
#     "residual sugar": Column(float, Check.in_range(0, 100), nullable=False),
#     "chlorides": Column(float, Check.in_range(0, 1), nullable=False),
#     "free sulfur dioxide": Column(float, Check.in_range(0, 300), nullable=False),
#     "total sulfur dioxide": Column(float, Check.in_range(0, 500), nullable=False),
#     "density": Column(float, Check.in_range(0.9, 1.1), nullable=False),
#     "pH": Column(float, Check.in_range(2, 5), nullable=False),
#     "sulphates": Column(float, Check.in_range(0, 3), nullable=False),
#     "alcohol": Column(float, Check.in_range(5, 20), nullable=False),
#     "quality": Column(int, Check.in_range(3, 9), nullable=False),
# })

# Validate the combined dataset
# try:
#     wine_schema.validate(wine_data[EXPECTED_COLUMNS])
#     print("\n[PASSED] Pandera schema validation successful!")
# except pa.errors.SchemaError as e:
#     print(f"\n[FAILED] Schema validation error: {e}")

# print("\n" + "="*60)
# print("DATA VALIDATION SUMMARY")
# print("="*60)
# print("""
# All validation checks completed:
#   1. File Format           - PASSED
#   2. Column Names          - PASSED  
#   3. Empty Observations    - PASSED
#   4. Missingness           - PASSED
#   5. Data Types            - PASSED
#   6. Duplicate Observations - PASSED (duplicates removed)
#   7. Outliers/Anomalies    - PASSED
#   8. Category Levels       - PASSED
#   9. Target Distribution   - INFO (class imbalance noted)
#  10. Target-Feature Corr   - PASSED (no leakage)
#  11. Feature-Feature Corr  - INFO (some high correlations)

# Data is ready for analysis. Proceeding with EDA...
# """)

```

### 2. Exploratory Data Analysis

```{python}
# # Quality distribution visualization
# fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# # Overall distribution
# wine_data['quality'].value_counts().sort_index().plot(kind='bar', ax=axes[0], color='steelblue')
# axes[0].set_title('Overall Wine Quality Distribution', fontsize=14, fontweight='bold')
# axes[0].set_xlabel('Quality Score')
# axes[0].set_ylabel('Count')
# axes[0].grid(True, alpha=0.3)

# # By wine type
# quality_by_type = wine_data.groupby(['wine_type', 'quality']).size().unstack(fill_value=0)
# quality_by_type.T.plot(kind='bar', ax=axes[1], color=['darkred', 'gold'], alpha=0.8)
# axes[1].set_title('Quality Distribution by Wine Type', fontsize=14, fontweight='bold')
# axes[1].set_xlabel('Quality Score')
# axes[1].set_ylabel('Count')
# axes[1].legend(['Red Wine', 'White Wine'])
# axes[1].grid(True, alpha=0.3)

# # Quality categories for Random Forest
# def categorize_quality(quality):
#     if quality <= 5:
#         return 'Low (3-5)'
#     elif quality <= 7:
#         return 'Medium (6-7)'
#     else:
#         return 'High (8-9)'

# wine_data['quality_category'] = wine_data['quality'].apply(categorize_quality)
# category_counts = wine_data['quality_category'].value_counts()
# category_counts.plot(kind='pie', ax=axes[2], autopct='%1.1f%%', colors=['#ff9999', '#66b3ff', '#99ff99'])
# axes[2].set_title('Quality Categories for Classification', fontsize=14, fontweight='bold')
# axes[2].set_ylabel('')

# plt.tight_layout()
# plt.show()

# print("Figure 1. Wine quality distribution analysis showing (a) overall quality score distribution, (b) distribution by wine type, and (c) quality categories for classification.")
# print("\nQuality Category Distribution:")
# print(category_counts)
# print(f"\nClass Imbalance Ratio: {category_counts.max() / category_counts.min():.2f}:1")
```

```{python}
# Feature correlations with quality
# feature_cols = wine_data.columns.drop(['quality', 'quality_category'])
# correlations = wine_data[feature_cols].corrwith(wine_data['quality']).sort_values(ascending=False)

# plt.figure(figsize=(10, 8))
# colors = ['green' if x > 0 else 'red' for x in correlations.values]
# correlations.plot(kind='barh', color=colors)
# plt.title('Feature Correlations with Wine Quality', fontsize=14, fontweight='bold')
# plt.xlabel('Correlation Coefficient')
# plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)
# plt.grid(True, alpha=0.3)
# plt.tight_layout()
# plt.show()

# print("Figure 2. Feature correlations with wine quality score. Green bars indicate positive correlations, red bars indicate negative correlations.")
# print("\nTop 5 Positive Correlations:")
# print(correlations.head())
# print("\nTop 5 Negative Correlations:")
# print(correlations.tail())
```

```{python}
# Feature correlation heatmap
# plt.figure(figsize=(12, 10))
# correlation_matrix = wine_data[feature_cols].corr()
# mask = np.triu(np.ones_like(correlation_matrix), k=1)
# sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', 
#             cmap='coolwarm', center=0, square=True, linewidths=1)
# plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold')
# plt.tight_layout()
# plt.show()

# print("Figure 3. Correlation heatmap showing relationships between all physicochemical features.")
```

### 3. Data Preprocessing for Random Forest

The wine quality dataset was prepared for analysis by separating the features (physicochemical properties of the wine) from the target variable (the quality category). The quality categories were encoded numerically so that the machine learning model could process them.

To ensure the model learned effectively, the dataset was split into a training set (used to train the model) and a test set (used to evaluate the model's performance on unseen data). The split was stratified, meaning the proportions of each quality category in the training and test sets match the overall dataset distribution. This helps the model fairly learn from all classes of wine quality.

```{python}
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# Load processed data

data = pd.read_csv("../data/processed/wine_data_cleaned.csv")

# Features and target

X = data.drop(columns=["quality", "quality_category"])
y = data["quality_category"]

# Encode target

le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Stratified train-test split

X_train, X_test, y_train, y_test = train_test_split(
X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

# Data split summary

train_samples = len(X_train)
test_samples = len(X_test)
n_features = X_train.shape[1]

# Class distributions

classes = le.classes_
train_counts = np.unique(y_train, return_counts=True)[1]
test_counts = np.unique(y_test, return_counts=True)[1]

train_percentages = (train_counts / train_samples * 100).round(1)
test_percentages = (test_counts / test_samples * 100).round(1)

# Convert NumPy types -> Python types
train_counts = [int(x) for x in train_counts]
test_counts = [int(x) for x in test_counts]

train_percentages = [float(x) for x in train_percentages]
test_percentages = [float(x) for x in test_percentages]

```
**Summary of Data Split**

| Dataset       | Samples | Features |
|---------------|--------|----------|
| Training Set  | `{python} train_samples` | `{python} n_features` |
| Test Set      | `{python} test_samples`  | `{python} n_features` |



**Class Distribution – Training Set**

| Quality Category | Samples | Percentage |
|-----------------|--------|-----------|
| `{python} classes[0]` | `{python} train_counts[0]` | `{python} train_percentages[0]`% |
| `{python} classes[1]` | `{python} train_counts[1]` | `{python} train_percentages[1]`% |
| `{python} classes[2]` | `{python} train_counts[2]` | `{python} train_percentages[2]`% |


**Class Distribution – Test Set**

| Quality Category | Samples | Percentage |
|-----------------|--------|-----------|
| `{python} classes[0]` | `{python} test_counts[0]` | `{python} test_percentages[0]`% |
| `{python} classes[1]` | `{python} test_counts[1]` | `{python} test_percentages[1]`% |
| `{python} classes[2]` | `{python} test_counts[2]` | `{python} test_percentages[2]`% |


This preprocessing ensured that the model would learn from all classes and could generalize well to new, unseen wines.

### 4. Random Forest Model Development

A Random Forest classifier, consisting of 100 decision trees, was trained on the processed data to predict wine quality. The model’s performance was evaluated using several metrics:

- Training Accuracy: Measures how well the model fits the training data.

- Test Accuracy: Measures how well the model generalizes to new, unseen data.

- Out-of-Bag (OOB) Score: An internal validation method for Random Forests, giving an unbiased estimate of model performance.

- Cross-Validation Accuracy: Performance measured by repeatedly splitting the training set into subsets to validate the model, ensuring stability of results.

```{python}
# Hidden chunk: train Random Forest and store metrics

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score

# Build and train Random Forest

rf_model = RandomForestClassifier(n_estimators=100, random_state=42, oob_score=True, n_jobs=1)
rf_model.fit(X_train, y_train)

# Predictions

y_pred_train = rf_model.predict(X_train)
y_pred_test = rf_model.predict(X_test)

# Performance metrics

train_accuracy = accuracy_score(y_train, y_pred_train)
test_accuracy = accuracy_score(y_test, y_pred_test)
oob_score = rf_model.oob_score_

cv_scores = cross_val_score(rf_model, X_train, y_train, cv=4, scoring='accuracy')
cv_mean = float(cv_scores.mean())
cv_std = float(cv_scores.std() * 2 ) # 95% CI approximation

```

**Table for Random Forest Performance**

| Metric                    | Value                                        |
| ------------------------- | -------------------------------------------- |
| Training Accuracy         | `{python} round(train_accuracy, 4)`                 |
| Test Accuracy             | `{python} round(test_accuracy, 4)`                  |
| Out-of-Bag (OOB) Score    | `{python} round(oob_score, 4)`                      |
| Cross-Validation Accuracy | `{python} round(cv_mean, 4)` ± `{python} round(cv_std, 4)` |


These results indicate that the model fits the training data very well while maintaining good predictive performance on unseen data. This demonstrates that the Random Forest classifier is effective in predicting wine quality based on physicochemical properties.

```{python}
# Feature Importance Analysis - Key advantage of Random Forest
# feature_importance = pd.DataFrame({
#     'feature': X.columns,
#     'importance': rf_model.feature_importances_
# }).sort_values('importance', ascending=False)

# plt.figure(figsize=(10, 8))
# colors = plt.cm.viridis(feature_importance['importance'] / feature_importance['importance'].max())
# plt.barh(range(len(feature_importance)), feature_importance['importance'], color=colors)
# plt.yticks(range(len(feature_importance)), feature_importance['feature'])
# plt.xlabel('Feature Importance', fontsize=12)
# plt.title('Random Forest Feature Importance Analysis', fontsize=14, fontweight='bold')
# plt.gca().invert_yaxis()
# plt.grid(True, alpha=0.3)
# plt.tight_layout()
# plt.show()

# print("Figure 4. Random Forest feature importance ranking showing the relative contribution of each physicochemical property to wine quality prediction.")
# print("\nTable 1. Top 5 Most Important Features:")
# print(feature_importance.head().to_string(index=False))
# print(f"\nCumulative importance of top 5 features: {feature_importance.head()['importance'].sum():.3f}")
```

```{python}
# Confusion Matrix for initial Random Forest
# cm = confusion_matrix(y_test, y_pred_test)
# plt.figure(figsize=(8, 6))
# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
#             xticklabels=label_encoder.classes_,
#             yticklabels=label_encoder.classes_)
# plt.title('Random Forest Confusion Matrix (Initial Model)', fontsize=14, fontweight='bold')
# plt.xlabel('Predicted')
# plt.ylabel('Actual')
# plt.tight_layout()
# plt.show()

# print("Figure 5. Confusion matrix for the initial Random Forest model showing classification performance across quality categories.")
# print("\nTable 2. Classification Report:")
# print(classification_report(y_test, y_pred_test, target_names=label_encoder.classes_))
```

### 5. Baseline Random Forest: Confusion Matrix and Classification Report

The following script is used to compute the confusion matrix and classification report on the test set:

```bash
python scripts/05_evaluate_using_confusion_matrix.py \
  --input-csv data/processed/wine_data_cleaned.csv \
  --model-path results/models/rf_wine_models.pkl \
  --output-dir results/evaluation
```

```{python}
from IPython.display import Image, display

display(Image("results/evaluation/confusion_matrix_random_forest_initial.png"))
```

Figure 1. Confusion matrix for the baseline Random Forest model, showing
classification performance across the quality categories.

```{python}
with open("results/evaluation/classification_report_random_forest_initial.txt") as f:
    print(f.read())
```

Table 1. Classification report (precision, recall, F1-score, support)
for the baseline Random Forest model.

### 6. Hyperparameter Tuning and Optimized Random Forest

To improve on the baseline Random Forest model, we perform hyperparameter
tuning using `GridSearchCV`. The tuning script

```bash
python scripts/07_tune_random_forest_hyperparameters.py \
  --input-csv data/processed/wine_data_cleaned.csv \
  --output-model results/models/rf_wine_model_optimized.pkl \
  --output-dir results/evaluation
```

- 6.1 Best Hyperparameters and Cross-Validation Performance

```{python}
with open("results/evaluation/rf_hyperparameter_tuning_results.txt") as f:
print(f.read())
```

Table 2. Summary of Random Forest hyperparameter tuning, including the
best hyperparameter combination and its cross-validation accuracy.

- 6.2 Confusion Matrix for the Optimized Model

```{python}
from IPython.display import Image, display
display(Image("results/evaluation/confusion_matrix_random_forest_optimized.png"))
```

Figure 3. Confusion matrix for the optimized Random Forest model after
hyperparameter tuning.

Compared to the baseline model in Figure 1, this confusion matrix shows
how the tuned model redistributes errors across the quality categories
and whether overall test performance improves. This allows us to assess
whether the additional model complexity from tuning leads to meaningful
gains in predictive accuracy.

```{python}
# Final Results Summary
# final_results = pd.DataFrame({
#     'Model': ['Baseline (Most Frequent Class)', 'Random Forest (Initial)', 'Random Forest (Optimized)', 
#               'Best Alternative (Gradient Boosting)'],
#     'Test Accuracy': [
#         max(np.bincount(y_test)) / len(y_test),  # Baseline
#         test_accuracy,  # Initial RF
#         accuracy_optimized,  # Optimized RF
#         results['Gradient Boosting']['Accuracy']  # Best alternative
#     ],
#     'F1-Score': [
#         np.nan,  # Baseline
#         results['Random Forest (Initial)']['F1-Score'],
#         results['Random Forest (Optimized)']['F1-Score'],
#         results['Gradient Boosting']['F1-Score']
#     ]
# })

# print("\n" + "="*60)
# print("FINAL RESULTS SUMMARY")
# print("="*60)
# print(final_results.to_string(index=False))

# # Key insights
# print("\n" + "="*60)
# print("KEY INSIGHTS FROM RANDOM FOREST ANALYSIS")
# print("="*60)

# print("\n1. MODEL PERFORMANCE:")
# print(f"   - Random Forest achieved {accuracy_optimized:.1%} accuracy")
# print(f"   - Improvement over baseline: {(accuracy_optimized - final_results.iloc[0]['Test Accuracy'])*100:.1f}%")
# print(f"   - Outperformed next best model by: {(accuracy_optimized - results['Gradient Boosting']['Accuracy'])*100:.1f}%")

# print("\n2. MOST IMPORTANT FEATURES FOR WINE QUALITY:")
# for i, row in feature_importance.head(3).iterrows():
#     print(f"   {i+1}. {row['feature']}: {row['importance']:.3f}")

# print("\n3. OPTIMAL RANDOM FOREST CONFIGURATION:")
# for param, value in rf_grid.best_params_.items():
#     print(f"   - {param}: {value}")

# print("\n4. MODEL ADVANTAGES DEMONSTRATED:")
# print("   - No feature scaling required")
# print("   - Captured non-linear relationships")
# print("   - Provided interpretable feature importance")
# print("   - Handled outliers in chemical measurements")
# print("   - OOB score provided unbiased error estimate")
```

### Discussion

Our Random Forest classifier successfully predicted wine quality from physicochemical properties, validating our hypothesis that ensemble learning methods are well-suited for this domain.

#### Why Random Forest Excelled:

1. **Ensemble Advantage**: By aggregating predictions from multiple decision trees, Random Forest reduced overfitting and improved generalization compared to single models.

2. **Feature Interactions**: The model effectively captured complex interactions between chemical properties that influence wine quality.

3. **Robustness**: Random Forest handled the class imbalance and potential outliers in chemical measurements without extensive preprocessing.

#### Key Findings:

- **Alcohol content** emerged as the most important predictor (importance: ~0.147), aligning with wine industry knowledge
- **Volatile acidity** (importance: ~0.105) and **density** (importance: ~0.105) were also important predictors
- The initial Random Forest model (~74.6% accuracy) slightly outperformed the grid-search optimized model (~74.2%), suggesting the default parameters were already well-suited for this dataset
- Random Forest outperformed other classifiers: Gradient Boosting (~73.1%), Logistic Regression (~72.6%), and SVM (~59.8%)

#### Practical Applications:

1. **Quality Control**: Winemakers can use the model to predict quality during production
2. **Process Optimization**: Focus on controlling key chemical properties identified by feature importance
3. **Objective Assessment**: Complement subjective expert ratings with data-driven predictions

#### Limitations and Future Work:

1. **Dataset Scope**: Limited to Portuguese "Vinho Verde" wines
2. **Feature Set**: Additional sensory data could improve predictions
3. **Temporal Factors**: Wine aging effects not captured
4. **Class Imbalance**: High-quality wines (8-9) represent only ~3% of samples, making prediction of this class challenging
5. **Duplicate Removal Impact**: Removing 1,177 duplicate rows reduced accuracy from ~82.5% to ~74.6%, suggesting the duplicates may have been artificially inflating model performance
6. **Future Directions**: 
   - Extend to other wine regions and varieties
   - Incorporate temporal data and aging models
   - Address class imbalance with techniques like SMOTE
   - Develop real-time quality monitoring systems

## Conclusion

This project successfully demonstrated that Random Forest is an effective choice for predicting wine quality from physicochemical properties. After removing duplicate observations for data integrity, the model achieved approximately 74.6% accuracy, significantly outperforming the baseline (~59.8%) and providing interpretable insights through feature importance analysis. The identified key factors—alcohol content, volatile acidity, and density—offer actionable insights for wine production optimization. Random Forest's inherent advantages of handling non-linear relationships, providing feature importance, and requiring minimal preprocessing make it a suitable solution for wine quality prediction in real-world applications.

## References

1. Cortez, P., Cerdeira, A., Almeida, F., Matos, T., & Reis, J. (2009). Modeling wine preferences by data mining from physicochemical properties. Decision Support Systems, 47(4), 547-553. https://doi.org/10.1016/j.dss.2009.05.016

2. Breiman, L. (2001). Random forests. Machine Learning, 45(1), 5-32. https://doi.org/10.1023/A:1010933404324

3. Dua, D. and Graff, C. (2019). UCI Machine Learning Repository. Irvine, CA: University of California, School of Information and Computer Science. https://archive.ics.uci.edu/ml

4. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., & Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12, 2825-2830.




