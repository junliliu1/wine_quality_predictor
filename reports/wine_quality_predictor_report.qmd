---
title: Predicting Wine Quality using Random Forest Classifier
author: "Purity Jangaya, Junli Liu, Jimmy Wang & Luis Alonso Alvarez"
date: "2025/11/20"
jupyter: python3

format:
  html:
    toc: true
    toc-depth: 4
    #output-file: index.html
  pdf:
    toc: true
    toc-depth: 4

bibliography: references.bib

execute:
  echo: false
  warning: false

editor: source
---

## Summary

This project implements a Random Forest classifier to predict wine quality based on physicochemical properties. Using the Wine Quality dataset from the UCI Machine Learning Repository (6,497 samples with 11 features, reduced to 5,320 after removing duplicates), we develop a robust prediction model that leverages the ensemble learning capabilities of Random Forest. Our analysis demonstrates that Random Forest effectively handles the non-linear relationships between chemical properties and wine quality, achieving an accuracy of approximately 74.6% on the test set. The model identifies alcohol content, volatile acidity, and density as the most influential factors in determining wine quality. This work provides valuable insights for wine producers to optimize production processes and maintain consistent quality standards.

## Introduction

Wine quality assessment traditionally relies on subjective evaluation by human experts. This project explores the potential of machine learning, specifically Random Forest classification, to predict wine quality from objective physicochemical measurements.

### Why Random Forest?

We selected Random Forest as our primary algorithm for several reasons:

1. **Ensemble Learning**: Combines multiple decision trees to reduce overfitting and improve generalization
2. **Feature Importance**: Provides built-in feature importance metrics for understanding wine quality factors
3. **Robustness**: Handles outliers and noise effectively without extensive preprocessing
4. **Non-linear Relationships**: Captures complex interactions between chemical properties
5. **No Scaling Required**: Works well with features at different scales
6. **Out-of-Bag (OOB) Error**: Provides unbiased error estimates without separate validation set

### Research Questions

1. Can Random Forest effectively predict wine quality from physicochemical properties?
2. Which chemical properties are most important for determining wine quality?
3. How does Random Forest performance compare to other classification methods?
4. What are the optimal hyperparameters for our Random Forest model?

## Methods

We used the Wine Quality Dataset from [@cortez2009modeling], comprising 6,497 wine samples (1,599 red, 4,898 white) with 11 physicochemical features and quality scores ranging from 3-9. After comprehensive data validation using Pandera-including schema checks, type verification, and removal of 1,177 duplicate transformations- the final analytical dataset contained 5,320 unique samples. We trained a Random Forest classifier [@breiman2001random] using scikit-learn [@pedregosa2011scikit]. The data was split 75/25 for training and testing, with 5-fold stratified cross-validation for model selection. Hyperparameters were optimized using grid search over number of trees (100-300), max depth (10-30, None), and minimum samples for splitting (2-10) and leaf nodes (1-4). Model performance was evaluated using accuracy, precision, recall, F1-score, and confusion matrices, with feature importance rankings were examined to identify key wine quality predictors.

### Data

**Dataset**: Wine Quality Dataset (@cortez2009modeling)
- **Red wine**: 1,599 samples
- **White wine**: 4,898 samples
<!-- Remove this column as they were duplicates -->
- **Total**: 6,497 samples
<!-- End of section to remove -->
- **Total**: 6,497 samples (5,320 after removing duplicates)


**Features (11 physicochemical properties)**:
1. Fixed acidity (g/dm³)
2. Volatile acidity (g/dm³)
3. Citric acid (g/dm³)
4. Residual sugar (g/dm³)
5. Chlorides (g/dm³)
6. Free sulfur dioxide (mg/dm³)
7. Total sulfur dioxide (mg/dm³)
8. Density (g/cm³)
9. pH
10. Sulphates (g/dm³)
11. Alcohol (% vol.)

**Target**: Quality score (3-9, originally 0-10 scale)

**Citation**: P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. Decision Support Systems, 47(4):547-553, 2009.

### Analysis Pipeline

1. **Data Preparation**
   - Load and combine red/white wine datasets
   - Handle class imbalance through stratified splitting
   - Create quality categories (Low: 3-5, Medium: 6-7, High: 8-9)

2. **Exploratory Data Analysis**
   - Analyze quality distribution
   - Examine feature correlations
   - Identify potential predictors

3. **Random Forest Implementation**
   - Build initial Random Forest with 100 trees
   - Analyze out-of-bag (OOB) error
   - Extract feature importances
   - Evaluate model performance

4. **Hyperparameter Optimization**
   - Grid search for optimal parameters:
     - n_estimators (number of trees)
     - max_depth (tree depth)
     - min_samples_split
     - min_samples_leaf
     - max_features

5. **Model Comparison**
   - Benchmark against Logistic Regression, SVM, and Gradient Boosting
   - Validate Random Forest superiority

6. **Final Evaluation**
   - Test set performance
   - Confusion matrix analysis
   - Feature importance interpretation

## Implementation

```{python}
#| echo: false
#| output: false

# Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import (
    classification_report, confusion_matrix, accuracy_score,
    precision_recall_fscore_support, roc_auc_score, roc_curve
)
import pandera as pa
from pandera import Column, Check, DataFrameSchema
import warnings
warnings.filterwarnings('ignore')

# Set random seed for reproducibility
np.random.seed(42)

# Set plotting style
sns.set_theme(style="whitegrid")
sns.set_palette("husl")

```

### 1. Data Loading and Preparation

To begin our analysis, we downloaded the two wine quality datasets provided by Cortez et al. (2009). These files contain measurements for red and white wine samples, including physicochemical properties and expert-assigned quality scores.

The data were downloaded using our automated script, which saves the raw files into the data/raw directory.

<!-- This I deleted as there are no track of this -->
<!--quarto render wine_quality_predictor_report.qmd-->
<!--  -->

The two datasets downloaded were:

- winequality-red.csv
- winequality-white.csv

These files form the foundation of the analysis that follows.
<!-- Changed this section in order to have three tables, also change the relative paths -->
```{python}
#| echo: false

# Load datasets
red = pd.read_csv("../data/raw/winequality-red.csv", sep=";").assign(wine_type=0)
white = pd.read_csv("../data/raw/winequality-white.csv", sep=";").assign(wine_type=1)
wine_data = pd.concat([red, white], ignore_index=True)

# Prepare inline variables
num_red = len(red)
num_white = len(white)
num_total = len(wine_data)

quality_counts = wine_data["quality"].value_counts().sort_index()
qualities = quality_counts.index.tolist()
counts = quality_counts.values.tolist()
percentages = [round(c / num_total * 100, 1) for c in counts]

missing_vals = wine_data.isnull().sum().tolist()
dtypes = [str(dt) for dt in wine_data.dtypes.tolist()]
columns = wine_data.columns.tolist()

```
```{python}
#| echo: false
#| label: tbl-dataset-overview
#| tbl-cap: "Summary of dataset composition after loading and combining red and white wine samples."

dataset_overview = pd.DataFrame({
    "Wine Type": ["Red", "White", "Total"],
    "Samples": [num_red, num_white, num_total],
    "Percentage": [
        round(num_red/num_total*100,1),
        round(num_white/num_total*100,1),
        100
    ]
})

dataset_overview
```

As shown in Table @tbl-dataset-overview, the dataset contains 1,599 red wines and 4,898 white wines, resulting in a combined total of 6,497 samples that form the basis for our subsequent cleaning, validation, and exploratory analysis.

```{python}
#| echo: false
#| label: tbl-quality-distribution
#| tbl-cap: "Distribution of wine quality scores in the combined dataset."

quality_table = pd.DataFrame({
    "Quality Score": qualities,
    "Count": counts,
    "Percentage": percentages
})

quality_table
```

Table @tbl-quality-distribution shows that most wines score between 5 and 7, underscoring the pronounced class imbalance that must be considered when selecting evaluation metrics and building predictive models.

```{python}
#| echo: false
#| label: tbl-data-quality
#| tbl-cap: "Summary of missing values and data types for each variable in the combined dataset."

data_quality_table = pd.DataFrame({
    "Variable": columns,
    "Missing Values": missing_vals,
    "Data Type": dtypes
})

data_quality_table
```

As detailed in Table @tbl-data-quality, the dataset contains no missing values and all variables have consistent types, which helps minimize preprocessing overhead and reduces the risk of introducing bias during model training.

<!-- Finished the change. I suggest eliminate the Table 3. Summary of missing values and data types -->

### 1. Data Validation

Before proceeding with analysis, we perform comprehensive data validation checks following the [Data Validation Checklist](https://ubc-dsci.github.io/reproducible-and-trustworthy-workflows-for-data-science/lectures/130-data-validation.html#data-validation-checklist) to ensure data quality and integrity.
<!-- TODO: Consider removing this section -->

<!-- End of section to remove -->

We validated the raw red and white wine datasets prior to analysis by confirming correct file structure, column names, and data types, and verifying the absence of missing or empty observations. Duplicate records (1,177 rows) were removed from the combined dataset, resulting in 5,320 unique samples. All features fell within expected ranges, category levels were valid, and the target variable showed a highly imbalanced—but expected—quality distribution. Correlation checks indicated no problematic collinearity or data leakage issues.


### 2. Exploratory Data Analysis

Our exploratory analysis reveals key insights about the wine quality dataset:

**Quality Distribution:** As shown in @fig-quality-distribution, wine quality scores follow a normal distribution centered around scores 5-6, with very few wines receiving extreme ratings (3 or 9). The dataset exhibits a significant class imbalance, with medium-quality wines (scores 6-7) comprising the majority of samples. White wines show a broader distribution across quality scores compared to red wines, which are more concentrated in the 5-6 range.

**Feature-Quality Relationships:** @fig-feature-correlations demonstrates that alcohol content exhibits the strongest positive correlation with wine quality, suggesting that higher alcohol wines tend to receive better ratings. Conversely, volatile acidity shows the strongest negative correlation, indicating that wines with higher acetic acid levels are rated lower. Other notable positive correlates include citric acid and sulphates, while density and chlorides show negative associations with quality.

**Feature Intercorrelations:** The correlation heatmap in @fig-correlation-heatmap reveals important relationships between physicochemical properties. Strong positive correlations exist between free and total sulfur dioxide, and between density and residual sugar. Alcohol shows a strong negative correlation with density, which is chemically expected. These intercorrelations suggest potential multicollinearity that may need to be addressed in our modeling approach, particularly when using linear methods.

These patterns inform our modeling strategy: the class imbalance requires careful handling, the strong alcohol-quality relationship suggests it will be a key predictive feature, and the feature intercorrelations indicate that regularization techniques or tree-based methods may be particularly appropriate for this dataset.

![Quality Categories for Classification](../results/eda/quality_distributions.png){#fig-quality-distribution}

@fig-quality-distribution shows the distribution of wine quality scores. Most wines receive medium quality scores (5-7), with very few at the extremes.

![Feature Correlations with Wine Quality](../results/eda/feature_correlations.png){#fig-feature-correlations}

@fig-feature-correlations reveals that alcohol content shows the strongest positive correlation with quality, while volatile acidity shows the strongest negative correlation.

![Feature Correlation Heatmap](../results/eda/correlation_heatmap.png){#fig-correlation-heatmap}

@fig-correlation-heatmap displays the correlations between all physicochemical features, helping identify potential multicollinearity issues for our models.

### 3. Data Preprocessing for Random Forest

The wine quality dataset was prepared for analysis by separating the features (physicochemical properties of the wine) from the target variable (the quality category). The quality categories were encoded numerically so that the machine learning model could process them.

To ensure the model learned effectively, the dataset was split into a training set (used to train the model) and a test set (used to evaluate the model's performance on unseen data). The split was stratified, meaning the proportions of each quality category in the training and test sets match the overall dataset distribution. This helps the model fairly learn from all classes of wine quality.

```{python}
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

# Load processed data

data = pd.read_csv("../data/processed/wine_data_cleaned.csv")

# Features and target

X = data.drop(columns=["quality", "quality_category"])
y = data["quality_category"]

# Encode target

le = LabelEncoder()
y_encoded = le.fit_transform(y)

# Stratified train-test split

X_train, X_test, y_train, y_test = train_test_split(
X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

# Data split summary

train_samples = len(X_train)
test_samples = len(X_test)
n_features = X_train.shape[1]

# Class distributions

classes = le.classes_
train_counts = np.unique(y_train, return_counts=True)[1]
test_counts = np.unique(y_test, return_counts=True)[1]

train_percentages = (train_counts / train_samples * 100).round(1)
test_percentages = (test_counts / test_samples * 100).round(1)

# Convert NumPy types -> Python types
train_counts = [int(x) for x in train_counts]
test_counts = [int(x) for x in test_counts]

train_percentages = [float(x) for x in train_percentages]
test_percentages = [float(x) for x in test_percentages]

```
**Summary of Data Split**

| Dataset       | Samples | Features |
|---------------|--------|----------|
| Training Set  | `{python} train_samples` | `{python} n_features` |
| Test Set      | `{python} test_samples`  | `{python} n_features` |



**Class Distribution – Training Set**

| Quality Category | Samples | Percentage |
|-----------------|--------|-----------|
| `{python} classes[0]` | `{python} train_counts[0]` | `{python} train_percentages[0]`% |
| `{python} classes[1]` | `{python} train_counts[1]` | `{python} train_percentages[1]`% |
| `{python} classes[2]` | `{python} train_counts[2]` | `{python} train_percentages[2]`% |


**Class Distribution – Test Set**

| Quality Category | Samples | Percentage |
|-----------------|--------|-----------|
| `{python} classes[0]` | `{python} test_counts[0]` | `{python} test_percentages[0]`% |
| `{python} classes[1]` | `{python} test_counts[1]` | `{python} test_percentages[1]`% |
| `{python} classes[2]` | `{python} test_counts[2]` | `{python} test_percentages[2]`% |


This preprocessing ensured that the model would learn from all classes and could generalize well to new, unseen wines.

### 4. Random Forest Model Development

A Random Forest classifier, consisting of 100 decision trees, was trained on the processed data to predict wine quality. The model’s performance was evaluated using several metrics:

- Training Accuracy: Measures how well the model fits the training data.

- Test Accuracy: Measures how well the model generalizes to new, unseen data.

- Out-of-Bag (OOB) Score: An internal validation method for Random Forests, giving an unbiased estimate of model performance.

- Cross-Validation Accuracy: Performance measured by repeatedly splitting the training set into subsets to validate the model, ensuring stability of results.

```{python}
# Hidden chunk: train Random Forest and store metrics

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score

# Build and train Random Forest

rf_model = RandomForestClassifier(n_estimators=100, random_state=42, oob_score=True, n_jobs=1)
rf_model.fit(X_train, y_train)

# Predictions

y_pred_train = rf_model.predict(X_train)
y_pred_test = rf_model.predict(X_test)

# Performance metrics

train_accuracy = accuracy_score(y_train, y_pred_train)
test_accuracy = accuracy_score(y_test, y_pred_test)
oob_score = rf_model.oob_score_

cv_scores = cross_val_score(rf_model, X_train, y_train, cv=4, scoring='accuracy')
cv_mean = float(cv_scores.mean())
cv_std = float(cv_scores.std() * 2 ) # 95% CI approximation

```

**Table for Random Forest Performance**

| Metric                    | Value                                        |
| ------------------------- | -------------------------------------------- |
| Training Accuracy         | `{python} round(train_accuracy, 4)`                 |
| Test Accuracy             | `{python} round(test_accuracy, 4)`                  |
| Out-of-Bag (OOB) Score    | `{python} round(oob_score, 4)`                      |
| Cross-Validation Accuracy | `{python} round(cv_mean, 4)` ± `{python} round(cv_std, 4)` |


These results indicate that the model fits the training data very well while maintaining good predictive performance on unseen data. This demonstrates that the Random Forest classifier is effective in predicting wine quality based on physicochemical properties.

```{python}
# Feature Importance Analysis - Key advantage of Random Forest
# feature_importance = pd.DataFrame({
#     'feature': X.columns,
#     'importance': rf_model.feature_importances_
# }).sort_values('importance', ascending=False)

# plt.figure(figsize=(10, 8))
# colors = plt.cm.viridis(feature_importance['importance'] / feature_importance['importance'].max())
# plt.barh(range(len(feature_importance)), feature_importance['importance'], color=colors)
# plt.yticks(range(len(feature_importance)), feature_importance['feature'])
# plt.xlabel('Feature Importance', fontsize=12)
# plt.title('Random Forest Feature Importance Analysis', fontsize=14, fontweight='bold')
# plt.gca().invert_yaxis()
# plt.grid(True, alpha=0.3)
# plt.tight_layout()
# plt.show()

# print("Figure 4. Random Forest feature importance ranking showing the relative contribution of each physicochemical property to wine quality prediction.")
# print("\nTable 1. Top 5 Most Important Features:")
# print(feature_importance.head().to_string(index=False))
# print(f"\nCumulative importance of top 5 features: {feature_importance.head()['importance'].sum():.3f}")
```

```{python}
# Confusion Matrix for initial Random Forest
# cm = confusion_matrix(y_test, y_pred_test)
# plt.figure(figsize=(8, 6))
# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
#             xticklabels=label_encoder.classes_,
#             yticklabels=label_encoder.classes_)
# plt.title('Random Forest Confusion Matrix (Initial Model)', fontsize=14, fontweight='bold')
# plt.xlabel('Predicted')
# plt.ylabel('Actual')
# plt.tight_layout()
# plt.show()

# print("Figure 5. Confusion matrix for the initial Random Forest model showing classification performance across quality categories.")
# print("\nTable 2. Classification Report:")
# print(classification_report(y_test, y_pred_test, target_names=label_encoder.classes_))
```

### 5. Random Forest Hyperparameter Optimization

```{python}
# Define parameter grid for Random Forest
# print("Starting Random Forest Hyperparameter Tuning...")
# print("This may take several minutes...\n")

# param_grid = {
#     'n_estimators': [100, 200, 300],
#     'max_depth': [10, 20, 30, None],
#     'min_samples_split': [2, 5, 10],
#     'min_samples_leaf': [1, 2, 4],
#     'max_features': ['sqrt', 'log2', None]
# }

# Create GridSearchCV object
# rf_grid = GridSearchCV(
#     RandomForestClassifier(random_state=42, oob_score=True, n_jobs=-1),
#     param_grid,
#     cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
#     scoring='accuracy',
#     n_jobs=-1,
#     verbose=1
# )

# Fit GridSearchCV
# rf_grid.fit(X_train, y_train)

# print(f"\nBest parameters found:")
# for param, value in rf_grid.best_params_.items():
#     print(f"  {param}: {value}")

# print(f"\nBest cross-validation score: {rf_grid.best_score_:.4f}")
```

```{python}
# Evaluate optimized Random Forest
# rf_optimized = rf_grid.best_estimator_
# y_pred_optimized = rf_optimized.predict(X_test)

# # Calculate metrics
# accuracy_optimized = accuracy_score(y_test, y_pred_optimized)
# precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred_optimized, average='weighted')

# print("Optimized Random Forest Performance:")
# print(f"Test Accuracy: {accuracy_optimized:.4f}")
# print(f"Precision: {precision:.4f}")
# print(f"Recall: {recall:.4f}")
# print(f"F1-Score: {f1:.4f}")
# print(f"\nImprovement over initial model: {(accuracy_optimized - test_accuracy)*100:.2f}%")

# # Confusion matrix for optimized model
# cm_optimized = confusion_matrix(y_test, y_pred_optimized)
# plt.figure(figsize=(8, 6))
# sns.heatmap(cm_optimized, annot=True, fmt='d', cmap='Greens',
#             xticklabels=label_encoder.classes_,
#             yticklabels=label_encoder.classes_)
# plt.title('Random Forest Confusion Matrix (Optimized Model)', fontsize=14, fontweight='bold')
# plt.xlabel('Predicted')
# plt.ylabel('Actual')
# plt.tight_layout()
# plt.show()

# print("\nFigure 6. Confusion matrix for the optimized Random Forest model after hyperparameter tuning.")
```

### 6. Model Comparison - Validating Random Forest Choice

```{python}
# Compare Random Forest with other models
# print("Comparing Random Forest with other classifiers...\n")

# models = {
#     'Random Forest (Optimized)': rf_optimized,
#     'Random Forest (Initial)': rf_model,
#     'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),
#     'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),
#     'SVM': SVC(kernel='rbf', random_state=42, probability=True)
# }

# results = {}
# for name, model in models.items():
#     if 'Random Forest' not in name:  # Skip RF models as they're already trained
#         model.fit(X_train, y_train)
    
#     y_pred = model.predict(X_test)
#     accuracy = accuracy_score(y_test, y_pred)
#     precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')
    
#     results[name] = {
#         'Accuracy': accuracy,
#         'Precision': precision,
#         'Recall': recall,
#         'F1-Score': f1
#     }
    
#     print(f"{name}:")
#     print(f"  Accuracy: {accuracy:.4f}")
#     print(f"  F1-Score: {f1:.4f}\n")
```

```{python}
# Visualize model comparison
# results_df = pd.DataFrame(results).T

# fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# # Bar plot comparison
# results_df.plot(kind='bar', ax=axes[0], width=0.8)
# axes[0].set_title('Model Performance Comparison', fontsize=14, fontweight='bold')
# axes[0].set_xlabel('Model')
# axes[0].set_ylabel('Score')
# axes[0].legend(loc='lower right')
# axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha='right')
# axes[0].grid(True, alpha=0.3)
# axes[0].set_ylim([0.5, 1.0])

# # Radar chart for Random Forest models
# categories = list(results_df.columns)
# rf_scores = results_df.loc['Random Forest (Optimized)'].values
# rf_init_scores = results_df.loc['Random Forest (Initial)'].values

# angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False)
# angles = np.concatenate((angles, [angles[0]]))
# rf_scores = np.concatenate((rf_scores, [rf_scores[0]]))
# rf_init_scores = np.concatenate((rf_init_scores, [rf_init_scores[0]]))

# ax = plt.subplot(122, projection='polar')
# ax.plot(angles, rf_scores, 'o-', linewidth=2, label='RF Optimized', color='green')
# ax.fill(angles, rf_scores, alpha=0.25, color='green')
# ax.plot(angles, rf_init_scores, 'o-', linewidth=2, label='RF Initial', color='blue')
# ax.fill(angles, rf_init_scores, alpha=0.25, color='blue')
# ax.set_xticks(angles[:-1])
# ax.set_xticklabels(categories)
# ax.set_ylim([0.5, 1.0])
# ax.set_title('Random Forest Performance Metrics', fontsize=14, fontweight='bold', y=1.08)
# ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
# ax.grid(True)

# plt.tight_layout()
# plt.show()

# print("Figure 7. Model performance comparison: (a) bar chart comparing all classifiers across metrics, (b) radar chart showing Random Forest performance profile.")
# print("\nTable 3. Performance Summary:")
# print(results_df.round(4).to_string())
```

### 7. Random Forest Deep Dive - Tree Visualization and Analysis

```{python}
# Analyze the effect of number of trees
# n_trees_range = range(10, 301, 10)
# oob_scores = []
# test_scores = []

# print("Analyzing impact of forest size...")
# for n_trees in n_trees_range:
#     rf_temp = RandomForestClassifier(
#         n_estimators=n_trees,
#         **{k: v for k, v in rf_grid.best_params_.items() if k != 'n_estimators'},
#         random_state=42,
#         oob_score=True,
#         n_jobs=-1
#     )
#     rf_temp.fit(X_train, y_train)
#     oob_scores.append(rf_temp.oob_score_)
#     test_scores.append(rf_temp.score(X_test, y_test))

# plt.figure(figsize=(10, 6))
# plt.plot(n_trees_range, oob_scores, label='OOB Score', linewidth=2)
# plt.plot(n_trees_range, test_scores, label='Test Score', linewidth=2)
# plt.xlabel('Number of Trees', fontsize=12)
# plt.ylabel('Accuracy', fontsize=12)
# plt.title('Random Forest Performance vs. Number of Trees', fontsize=14, fontweight='bold')
# plt.legend(loc='lower right')
# plt.grid(True, alpha=0.3)
# plt.tight_layout()
# plt.show()

# print(f"\nFigure 8. Effect of forest size on model performance showing OOB score and test accuracy as number of trees increases.")
# print(f"\nOptimal number of trees (based on test score): {n_trees_range[np.argmax(test_scores)]}")
# print(f"Maximum test accuracy achieved: {max(test_scores):.4f}")
```

```{python}
# Feature importance stability across multiple runs
# n_runs = 10
# importance_matrix = np.zeros((n_runs, len(X.columns)))

# print("Analyzing feature importance stability...")
# for i in range(n_runs):
#     rf_temp = RandomForestClassifier(
#         **rf_grid.best_params_,
#         random_state=i,
#         oob_score=True,
#         n_jobs=-1
#     )
#     rf_temp.fit(X_train, y_train)
#     importance_matrix[i] = rf_temp.feature_importances_

# # Calculate mean and std of feature importances
# importance_mean = importance_matrix.mean(axis=0)
# importance_std = importance_matrix.std(axis=0)

# # Create dataframe for visualization
# importance_stability = pd.DataFrame({
#     'feature': X.columns,
#     'mean_importance': importance_mean,
#     'std_importance': importance_std
# }).sort_values('mean_importance', ascending=False)

# plt.figure(figsize=(10, 8))
# plt.barh(range(len(importance_stability)), importance_stability['mean_importance'], 
#          xerr=importance_stability['std_importance'], color='forestgreen', alpha=0.7)
# plt.yticks(range(len(importance_stability)), importance_stability['feature'])
# plt.xlabel('Feature Importance (Mean ± Std)', fontsize=12)
# plt.title('Random Forest Feature Importance Stability Analysis', fontsize=14, fontweight='bold')
# plt.gca().invert_yaxis()
# plt.grid(True, alpha=0.3)
# plt.tight_layout()
# plt.show()

# print("\nFigure 9. Feature importance stability analysis across 10 random forest runs showing mean importance with standard deviation error bars.")
# print("\nTable 4. Feature Importance Stability (Top 5):")
# print(importance_stability.head().to_string(index=False))
```

## Results & Discussion

```{python}
# Final Results Summary
# final_results = pd.DataFrame({
#     'Model': ['Baseline (Most Frequent Class)', 'Random Forest (Initial)', 'Random Forest (Optimized)', 
#               'Best Alternative (Gradient Boosting)'],
#     'Test Accuracy': [
#         max(np.bincount(y_test)) / len(y_test),  # Baseline
#         test_accuracy,  # Initial RF
#         accuracy_optimized,  # Optimized RF
#         results['Gradient Boosting']['Accuracy']  # Best alternative
#     ],
#     'F1-Score': [
#         np.nan,  # Baseline
#         results['Random Forest (Initial)']['F1-Score'],
#         results['Random Forest (Optimized)']['F1-Score'],
#         results['Gradient Boosting']['F1-Score']
#     ]
# })

# print("\n" + "="*60)
# print("FINAL RESULTS SUMMARY")
# print("="*60)
# print(final_results.to_string(index=False))

# # Key insights
# print("\n" + "="*60)
# print("KEY INSIGHTS FROM RANDOM FOREST ANALYSIS")
# print("="*60)

# print("\n1. MODEL PERFORMANCE:")
# print(f"   - Random Forest achieved {accuracy_optimized:.1%} accuracy")
# print(f"   - Improvement over baseline: {(accuracy_optimized - final_results.iloc[0]['Test Accuracy'])*100:.1f}%")
# print(f"   - Outperformed next best model by: {(accuracy_optimized - results['Gradient Boosting']['Accuracy'])*100:.1f}%")

# print("\n2. MOST IMPORTANT FEATURES FOR WINE QUALITY:")
# for i, row in feature_importance.head(3).iterrows():
#     print(f"   {i+1}. {row['feature']}: {row['importance']:.3f}")

# print("\n3. OPTIMAL RANDOM FOREST CONFIGURATION:")
# for param, value in rf_grid.best_params_.items():
#     print(f"   - {param}: {value}")

# print("\n4. MODEL ADVANTAGES DEMONSTRATED:")
# print("   - No feature scaling required")
# print("   - Captured non-linear relationships")
# print("   - Provided interpretable feature importance")
# print("   - Handled outliers in chemical measurements")
# print("   - OOB score provided unbiased error estimate")
```

### Discussion

Our Random Forest classifier successfully predicted wine quality from physicochemical properties, validating our hypothesis that ensemble learning methods are well-suited for this domain.

#### Why Random Forest Excelled:

1. **Ensemble Advantage**: By aggregating predictions from multiple decision trees, Random Forest reduced overfitting and improved generalization compared to single models.

2. **Feature Interactions**: The model effectively captured complex interactions between chemical properties that influence wine quality.

3. **Robustness**: Random Forest handled the class imbalance and potential outliers in chemical measurements without extensive preprocessing.

#### Key Findings:

- **Alcohol content** emerged as the most important predictor (importance: ~0.147), aligning with wine industry knowledge
- **Volatile acidity** (importance: ~0.105) and **density** (importance: ~0.105) were also important predictors
- The initial Random Forest model (~74.6% accuracy) slightly outperformed the grid-search optimized model (~74.2%), suggesting the default parameters were already well-suited for this dataset
- Random Forest outperformed other classifiers: Gradient Boosting (~73.1%), Logistic Regression (~72.6%), and SVM (~59.8%)

#### Practical Applications:

1. **Quality Control**: Winemakers can use the model to predict quality during production
2. **Process Optimization**: Focus on controlling key chemical properties identified by feature importance
3. **Objective Assessment**: Complement subjective expert ratings with data-driven predictions

#### Limitations and Future Work:

1. **Dataset Scope**: Limited to Portuguese "Vinho Verde" wines
2. **Feature Set**: Additional sensory data could improve predictions
3. **Temporal Factors**: Wine aging effects not captured
4. **Class Imbalance**: High-quality wines (8-9) represent only ~3% of samples, making prediction of this class challenging
5. **Duplicate Removal Impact**: Removing 1,177 duplicate rows reduced accuracy from ~82.5% to ~74.6%, suggesting the duplicates may have been artificially inflating model performance
6. **Future Directions**: 
   - Extend to other wine regions and varieties
   - Incorporate temporal data and aging models
   - Address class imbalance with techniques like SMOTE
   - Develop real-time quality monitoring systems

## Conclusion

This project successfully demonstrated that Random Forest is an effective choice for predicting wine quality from physicochemical properties. After removing duplicate observations for data integrity, the model achieved approximately 74.6% accuracy, significantly outperforming the baseline (~59.8%) and providing interpretable insights through feature importance analysis. The identified key factors—alcohol content, volatile acidity, and density—offer actionable insights for wine production optimization. Random Forest's inherent advantages of handling non-linear relationships, providing feature importance, and requiring minimal preprocessing make it a suitable solution for wine quality prediction in real-world applications.

## References

::: {#refs}
:::




